{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "StemGNN_transformer_with_clusters_and_corrmap.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SuhwanChung/CE7454/blob/chunhung/StemGNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eb_uVqXFigx-"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gWbvqs6euE8v",
        "outputId": "26f67bad-1d05-4785-c2da-7fc63e846f4f"
      },
      "source": [
        "# choose your runtime (GPU/None)\n",
        "\n",
        "# edit CELL 5 LAST LINE TO CHANGE CLUSTER NUMBER AND CLUSTER PATH\n",
        "\n",
        "\n",
        "from google.colab import drive\n",
        "import os\n",
        "import glob\n",
        "import re\n",
        "import cv2\n",
        "from functools import partial\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from matplotlib import pyplot as plt\n",
        "from PIL import Image, ImageDraw\n",
        "import seaborn as sns\n",
        "sns.set_style('whitegrid')\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import pandas as pd\n",
        "import torchvision.transforms as T\n",
        "from datetime import datetime\n",
        "import json\n",
        "import time\n",
        "\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "wd = 'drive/MyDrive/CE7454 Demand forecast' #specify your project folder name\n",
        "\n",
        "if not os.path.exists(os.path.join(wd,'StemGNN')):\n",
        "  !git clone https://github.com/microsoft/StemGNN.git '{wd}/StemGNN'\n",
        "\n",
        "\n",
        "!pip install --upgrade pip\n",
        "!pip install -r '{wd}/StemGNN/requirements.txt'\n",
        "!pip install torch==1.7.1 torchvision==0.8.2 \n",
        "\n",
        "\n",
        "# restart to update torch version\n",
        "#os.kill(os.getpid(), 9)\n"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Requirement already satisfied: pip in /usr/local/lib/python3.7/dist-packages (21.3.1)\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
            "Requirement already satisfied: numpy==1.19.5 in /usr/local/lib/python3.7/dist-packages (from -r drive/MyDrive/CE7454 Demand forecast/StemGNN/requirements.txt (line 1)) (1.19.5)\n",
            "Requirement already satisfied: scipy==1.5.4 in /usr/local/lib/python3.7/dist-packages (from -r drive/MyDrive/CE7454 Demand forecast/StemGNN/requirements.txt (line 2)) (1.5.4)\n",
            "Requirement already satisfied: pandas==1.1.5 in /usr/local/lib/python3.7/dist-packages (from -r drive/MyDrive/CE7454 Demand forecast/StemGNN/requirements.txt (line 3)) (1.1.5)\n",
            "Requirement already satisfied: torch==1.7.1 in /usr/local/lib/python3.7/dist-packages (from -r drive/MyDrive/CE7454 Demand forecast/StemGNN/requirements.txt (line 4)) (1.7.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas==1.1.5->-r drive/MyDrive/CE7454 Demand forecast/StemGNN/requirements.txt (line 3)) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas==1.1.5->-r drive/MyDrive/CE7454 Demand forecast/StemGNN/requirements.txt (line 3)) (2018.9)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.7.1->-r drive/MyDrive/CE7454 Demand forecast/StemGNN/requirements.txt (line 4)) (3.10.0.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas==1.1.5->-r drive/MyDrive/CE7454 Demand forecast/StemGNN/requirements.txt (line 3)) (1.15.0)\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
            "Requirement already satisfied: torch==1.7.1 in /usr/local/lib/python3.7/dist-packages (1.7.1)\n",
            "Requirement already satisfied: torchvision==0.8.2 in /usr/local/lib/python3.7/dist-packages (0.8.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.7.1) (3.10.0.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch==1.7.1) (1.19.5)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.7/dist-packages (from torchvision==0.8.2) (7.1.2)\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K_TVTUdeLKrs",
        "outputId": "23e99f44-c016-4372-9075-6ac69c07747f"
      },
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "import glob\n",
        "import re\n",
        "import cv2\n",
        "from functools import partial\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from matplotlib import pyplot as plt\n",
        "from PIL import Image, ImageDraw\n",
        "import seaborn as sns\n",
        "sns.set_style('whitegrid')\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import pandas as pd\n",
        "import torchvision.transforms as T\n",
        "from datetime import datetime\n",
        "import json\n",
        "import time\n",
        "\n",
        "\n",
        "print(torch.__version__)\n",
        "wd = 'drive/MyDrive/CE7454 Demand forecast'\n",
        "%cd '{wd}/StemGNN'"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.7.1\n",
            "/content/drive/My Drive/CE7454 Demand forecast/StemGNN/drive/MyDrive/CE7454 Demand forecast/StemGNN\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "id": "0jcO229foSr9",
        "outputId": "3d34ebb9-fb0f-43fd-f727-d14693ecebd0"
      },
      "source": [
        "'''\n",
        "!pip install yahoofinancials\n",
        "import pandas as pd\n",
        "from yahoofinancials import YahooFinancials\n",
        "from shutil import copyfile\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "def main(in_ticker_csv, out_csv):\n",
        "    output_file_path = out_csv\n",
        "    temp_file_path = output_file_path + '.temp'\n",
        "    date_error_msg = 'Dates are not aligned when adding new company stock prices!\\n' + \\\n",
        "                 'Date in file: {}, Date to add: {}'\n",
        "\n",
        "    # Read company symbols\n",
        "    sp_df = pd.read_csv(in_ticker_csv, encoding = 'unicode_escape')\n",
        "    sp_500_symbols = sp_df.Ticker.values.tolist()\n",
        "\n",
        "    # Prepare the file to write stock prices\n",
        "    symbol_subset = sp_500_symbols[:4]\n",
        "    history = YahooFinancials(symbol_subset)\n",
        "    data = history.get_historical_price_data(start_date='2016-09-30',\n",
        "                                                  end_date='2021-09-30',\n",
        "                                                  time_interval='daily')\n",
        "    prices_df = pd.DataFrame({\n",
        "        a: {x['formatted_date']: x['adjclose'] for x in data[a]['prices']} for a in symbol_subset\n",
        "    })\n",
        "    with open(output_file_path, 'w') as wf:\n",
        "        wf.write(','.join(['date']) + '\\n')\n",
        "        for index, row in prices_df.iterrows():\n",
        "            wf.write(','.join([index]) + '\\n')\n",
        "    date_count = len(prices_df.index)\n",
        "\n",
        "    # Write stock prices of a sebset of companies at a time, or it may hang\n",
        "    batch_size = 1\n",
        "    for i in tqdm(range(0, len(sp_500_symbols), batch_size)):\n",
        "        # Query a subset of companies only\n",
        "        symbol_subset = sp_500_symbols[i:i+batch_size]\n",
        "        history = YahooFinancials(symbol_subset)\n",
        "        data = history.get_historical_price_data(start_date='2016-09-30',\n",
        "                                                      end_date='2021-09-30',\n",
        "                                                      time_interval='daily')\n",
        "        try:\n",
        "            prices_df = pd.DataFrame({\n",
        "                a: {x['formatted_date']: x['adjclose'] for x in data[a]['prices']} for a in symbol_subset\n",
        "            })\n",
        "        except:\n",
        "            print('Skipping for symbols {} as their yahoo queries are not responding'.format(symbol_subset))\n",
        "            continue\n",
        "\n",
        "        new_data_to_add = list(prices_df.iterrows())\n",
        "        if len(new_data_to_add) != date_count:\n",
        "            print('Skipping for symbols {} as they don\\'t have the price info for all the dates'.format(symbol_subset))\n",
        "            continue\n",
        "        \n",
        "        copyfile(output_file_path, temp_file_path)\n",
        "        with open(temp_file_path, 'r') as rf, open(output_file_path, 'w') as wf:\n",
        "            for i2, line in enumerate(rf.readlines()):\n",
        "                # Add more company symbols to the file\n",
        "                if i2 == 0:\n",
        "                    wf.write(line[:-1] + ',' + ','.join(symbol_subset) + '\\n')\n",
        "                # For each new company, add their stock prices\n",
        "                else:\n",
        "                    assert line[:-1].split(',')[0] == new_data_to_add[i2-1][0], date_error_msg.format(line[:-1].split(',')[0], new_data_to_add[i2-1][0])\n",
        "                    wf.write(line[:-1] + ',' + ','.join([str(t) for t in new_data_to_add[i2-1][1].values.tolist()]) + '\\n')\n",
        "\n",
        "    print('Done writing all company stock prices to file.')\n",
        "    '''\n"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"\\n!pip install yahoofinancials\\nimport pandas as pd\\nfrom yahoofinancials import YahooFinancials\\nfrom shutil import copyfile\\nfrom tqdm import tqdm\\n\\n\\ndef main(in_ticker_csv, out_csv):\\n    output_file_path = out_csv\\n    temp_file_path = output_file_path + '.temp'\\n    date_error_msg = 'Dates are not aligned when adding new company stock prices!\\n' +                  'Date in file: {}, Date to add: {}'\\n\\n    # Read company symbols\\n    sp_df = pd.read_csv(in_ticker_csv, encoding = 'unicode_escape')\\n    sp_500_symbols = sp_df.Ticker.values.tolist()\\n\\n    # Prepare the file to write stock prices\\n    symbol_subset = sp_500_symbols[:4]\\n    history = YahooFinancials(symbol_subset)\\n    data = history.get_historical_price_data(start_date='2016-09-30',\\n                                                  end_date='2021-09-30',\\n                                                  time_interval='daily')\\n    prices_df = pd.DataFrame({\\n        a: {x['formatted_date']: x['adjclose'] for x in data[a]['prices']} for a in symbol_subset\\n    })\\n    with open(output_file_path, 'w') as wf:\\n        wf.write(','.join(['date']) + '\\n')\\n        for index, row in prices_df.iterrows():\\n            wf.write(','.join([index]) + '\\n')\\n    date_count = len(prices_df.index)\\n\\n    # Write stock prices of a sebset of companies at a time, or it may hang\\n    batch_size = 1\\n    for i in tqdm(range(0, len(sp_500_symbols), batch_size)):\\n        # Query a subset of companies only\\n        symbol_subset = sp_500_symbols[i:i+batch_size]\\n        history = YahooFinancials(symbol_subset)\\n        data = history.get_historical_price_data(start_date='2016-09-30',\\n                                                      end_date='2021-09-30',\\n                                                      time_interval='daily')\\n        try:\\n            prices_df = pd.DataFrame({\\n                a: {x['formatted_date']: x['adjclose'] for x in data[a]['prices']} for a in symbol_subset\\n            })\\n        except:\\n            print('Skipping for symbols {} as their yahoo queries are not responding'.format(symbol_subset))\\n            continue\\n\\n        new_data_to_add = list(prices_df.iterrows())\\n        if len(new_data_to_add) != date_count:\\n            print('Skipping for symbols {} as they don't have the price info for all the dates'.format(symbol_subset))\\n            continue\\n        \\n        copyfile(output_file_path, temp_file_path)\\n        with open(temp_file_path, 'r') as rf, open(output_file_path, 'w') as wf:\\n            for i2, line in enumerate(rf.readlines()):\\n                # Add more company symbols to the file\\n                if i2 == 0:\\n                    wf.write(line[:-1] + ',' + ','.join(symbol_subset) + '\\n')\\n                # For each new company, add their stock prices\\n                else:\\n                    assert line[:-1].split(',')[0] == new_data_to_add[i2-1][0], date_error_msg.format(line[:-1].split(',')[0], new_data_to_add[i2-1][0])\\n                    wf.write(line[:-1] + ',' + ','.join([str(t) for t in new_data_to_add[i2-1][1].values.tolist()]) + '\\n')\\n\\n    print('Done writing all company stock prices to file.')\\n    \""
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uxtq7mgjocZZ"
      },
      "source": [
        "#main('/content/drive/My Drive/dataset/sp500/S&P 500 tickers.csv', '/content/drive/My Drive/dataset/sp500/sp500_prices_adjclose_30_09_16_30_09_21.csv')"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "id": "o_4fTYpLQ67W",
        "outputId": "6e1e768b-ebf8-4845-f7ae-c4dec8b61ec6"
      },
      "source": [
        "# config args\n",
        "'''\n",
        "args ={\n",
        "\n",
        "\n",
        "'train':True,\n",
        "'evaluate':True,\n",
        "'dataset':'sp500_prices_adjclose_30_09_16_30_09_21',\n",
        "'window_size':12,\n",
        "'horizon':3,\n",
        "'train_length': 7,\n",
        "'valid_length': 2,\n",
        "'test_length' :1,\n",
        "'epoch':50,\n",
        "'lr' :1e-4,\n",
        "'multi_layer':5,\n",
        "'device':'cuda',\n",
        "'validate_freq':1,\n",
        "'batch_size':32,\n",
        "'norm_method':'z_score',\n",
        "'optimizer':'RMSProp',\n",
        "'early_stop':False,\n",
        "'exponential_decay_step':5,\n",
        "'decay_rate' :0.5,\n",
        "'dropout_rate' :0.5,\n",
        "'leakyrelu_rate':0.2,\n",
        "}\n",
        "\n",
        "class AttrDict(dict):\n",
        "  def __init__(self,*args, **kwargs):\n",
        "    super().__init__(*args, **kwargs)\n",
        "    self.__dict__ = self\n",
        "\n",
        "args = AttrDict(args)\n",
        "'''"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"\\nargs ={\\n\\n\\n'train':True,\\n'evaluate':True,\\n'dataset':'sp500_prices_adjclose_30_09_16_30_09_21',\\n'window_size':12,\\n'horizon':3,\\n'train_length': 7,\\n'valid_length': 2,\\n'test_length' :1,\\n'epoch':50,\\n'lr' :1e-4,\\n'multi_layer':5,\\n'device':'cuda',\\n'validate_freq':1,\\n'batch_size':32,\\n'norm_method':'z_score',\\n'optimizer':'RMSProp',\\n'early_stop':False,\\n'exponential_decay_step':5,\\n'decay_rate' :0.5,\\n'dropout_rate' :0.5,\\n'leakyrelu_rate':0.2,\\n}\\n\\nclass AttrDict(dict):\\n  def __init__(self,*args, **kwargs):\\n    super().__init__(*args, **kwargs)\\n    self.__dict__ = self\\n\\nargs = AttrDict(args)\\n\""
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "foNwdujrJXIB"
      },
      "source": [
        "# config args\n",
        "\n",
        "args ={\n",
        "'train':True,\n",
        "'evaluate':True,\n",
        "'dataset':'sp500_prices_adjclose_30_09_16_30_09_21',\n",
        "#'data_dir': proj_dir + 'data/',\n",
        "'window_size':12,\n",
        "'horizon':3,\n",
        "'train_length': 7,\n",
        "'valid_length': 2,\n",
        "'test_length' :1,\n",
        "'epoch':50,\n",
        "'lr' :1e-4,\n",
        "'multi_layer':5,\n",
        "'device':'cuda',\n",
        "'validate_freq':1,\n",
        "'batch_size':8,\n",
        "'norm_method':'z_score',\n",
        "'optimizer':'RMSProp',\n",
        "'early_stop':False,\n",
        "'exponential_decay_step':5,\n",
        "'decay_rate' :0.5,\n",
        "'dropout_rate' :0.5,\n",
        "'leakyrelu_rate':0.2,\n",
        "}\n",
        "\n",
        "class AttrDict(dict):\n",
        "  def __init__(self,*args, **kwargs):\n",
        "    super().__init__(*args, **kwargs)\n",
        "    self.__dict__ = self\n",
        "args = AttrDict(args)\n",
        "\n",
        "\n",
        "# CHANGE CLUSTER HERE \n",
        "# cluster = 1 or 2 or 3 or 4\n",
        "# cluster path is the csv file with each stock and its cluster label\n",
        "cluster = 1\n",
        "cluster_path = \"/content/drive/MyDrive/Nanyang/class/CE7454/cluster.csv\""
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 142
        },
        "id": "-1R3az0PShhM",
        "outputId": "c6e9f78a-4aab-4f62-c979-7920354e904f"
      },
      "source": [
        "\n",
        "# GET training data\n",
        "\n",
        "\n",
        "print(f'Training configs: {args}')\n",
        "#data_file = os.path.join('dataset', args.dataset + '.csv')\n",
        "data_file = '/content/drive/My Drive/dataset/sp500/data.csv'\n",
        "\n",
        "'''\n",
        "result_train_file = os.path.join('output', args.dataset, 'train')\n",
        "result_test_file = os.path.join('output', args.dataset, 'test')\n",
        "if not os.path.exists(result_train_file):\n",
        "    os.makedirs(result_train_file)\n",
        "if not os.path.exists(result_test_file):\n",
        "    os.makedirs(result_test_file)\n",
        "\n",
        "# data = pd.read_csv(data_file).values\n",
        "data = pd.read_csv(data_file).drop('date',1).values\n",
        "# split data\n",
        "train_ratio = args.train_length / (args.train_length + args.valid_length + args.test_length)\n",
        "valid_ratio = args.valid_length / (args.train_length + args.valid_length + args.test_length)\n",
        "test_ratio = 1 - train_ratio - valid_ratio\n",
        "train_data = data[:int(train_ratio * len(data))]\n",
        "valid_data = data[int(train_ratio * len(data)):int((train_ratio + valid_ratio) * len(data))]\n",
        "test_data = data[int((train_ratio + valid_ratio) * len(data)):]\n",
        "'''"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training configs: {'train': True, 'evaluate': True, 'dataset': 'sp500_prices_adjclose_30_09_16_30_09_21', 'window_size': 12, 'horizon': 3, 'train_length': 7, 'valid_length': 2, 'test_length': 1, 'epoch': 50, 'lr': 0.0001, 'multi_layer': 5, 'device': 'cuda', 'validate_freq': 1, 'batch_size': 8, 'norm_method': 'z_score', 'optimizer': 'RMSProp', 'early_stop': False, 'exponential_decay_step': 5, 'decay_rate': 0.5, 'dropout_rate': 0.5, 'leakyrelu_rate': 0.2}\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"\\nresult_train_file = os.path.join('output', args.dataset, 'train')\\nresult_test_file = os.path.join('output', args.dataset, 'test')\\nif not os.path.exists(result_train_file):\\n    os.makedirs(result_train_file)\\nif not os.path.exists(result_test_file):\\n    os.makedirs(result_test_file)\\n\\n# data = pd.read_csv(data_file).values\\ndata = pd.read_csv(data_file).drop('date',1).values\\n# split data\\ntrain_ratio = args.train_length / (args.train_length + args.valid_length + args.test_length)\\nvalid_ratio = args.valid_length / (args.train_length + args.valid_length + args.test_length)\\ntest_ratio = 1 - train_ratio - valid_ratio\\ntrain_data = data[:int(train_ratio * len(data))]\\nvalid_data = data[int(train_ratio * len(data)):int((train_ratio + valid_ratio) * len(data))]\\ntest_data = data[int((train_ratio + valid_ratio) * len(data)):]\\n\""
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 256
        },
        "id": "S95rhT2ySiAG",
        "outputId": "610c1a3b-0db9-4936-f38d-22e015fbe9c5"
      },
      "source": [
        "pd.read_csv(data_file).drop('date',1).head()"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>MMM</th>\n",
              "      <th>ABT</th>\n",
              "      <th>ABBV</th>\n",
              "      <th>ABMD</th>\n",
              "      <th>ACN</th>\n",
              "      <th>ATVI</th>\n",
              "      <th>ADBE</th>\n",
              "      <th>AMD</th>\n",
              "      <th>AAP</th>\n",
              "      <th>AES</th>\n",
              "      <th>AFL</th>\n",
              "      <th>A</th>\n",
              "      <th>APD</th>\n",
              "      <th>AKAM</th>\n",
              "      <th>ALK</th>\n",
              "      <th>ALB</th>\n",
              "      <th>ARE</th>\n",
              "      <th>ALGN</th>\n",
              "      <th>ALLE</th>\n",
              "      <th>LNT</th>\n",
              "      <th>ALL</th>\n",
              "      <th>GOOGL</th>\n",
              "      <th>GOOG</th>\n",
              "      <th>MO</th>\n",
              "      <th>AMZN</th>\n",
              "      <th>AMCR</th>\n",
              "      <th>AEE</th>\n",
              "      <th>AAL</th>\n",
              "      <th>AEP</th>\n",
              "      <th>AXP</th>\n",
              "      <th>AIG</th>\n",
              "      <th>AMT</th>\n",
              "      <th>AWK</th>\n",
              "      <th>AMP</th>\n",
              "      <th>ABC</th>\n",
              "      <th>AME</th>\n",
              "      <th>AMGN</th>\n",
              "      <th>APH</th>\n",
              "      <th>ADI</th>\n",
              "      <th>ANSS</th>\n",
              "      <th>...</th>\n",
              "      <th>UNM</th>\n",
              "      <th>VLO</th>\n",
              "      <th>VTR</th>\n",
              "      <th>VRSN</th>\n",
              "      <th>VRSK</th>\n",
              "      <th>VZ</th>\n",
              "      <th>VRTX</th>\n",
              "      <th>VFC</th>\n",
              "      <th>VIAC</th>\n",
              "      <th>VTRS</th>\n",
              "      <th>V</th>\n",
              "      <th>VNO</th>\n",
              "      <th>VMC</th>\n",
              "      <th>WRB</th>\n",
              "      <th>WAB</th>\n",
              "      <th>WMT</th>\n",
              "      <th>WBA</th>\n",
              "      <th>DIS</th>\n",
              "      <th>WM</th>\n",
              "      <th>WAT</th>\n",
              "      <th>WEC</th>\n",
              "      <th>WFC</th>\n",
              "      <th>WELL</th>\n",
              "      <th>WST</th>\n",
              "      <th>WDC</th>\n",
              "      <th>WU</th>\n",
              "      <th>WRK</th>\n",
              "      <th>WY</th>\n",
              "      <th>WHR</th>\n",
              "      <th>WMB</th>\n",
              "      <th>WLTW</th>\n",
              "      <th>WYNN</th>\n",
              "      <th>XEL</th>\n",
              "      <th>XLNX</th>\n",
              "      <th>XYL</th>\n",
              "      <th>YUM</th>\n",
              "      <th>ZBRA</th>\n",
              "      <th>ZBH</th>\n",
              "      <th>ZION</th>\n",
              "      <th>ZTS</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>151.891006</td>\n",
              "      <td>38.611118</td>\n",
              "      <td>50.265553</td>\n",
              "      <td>128.580002</td>\n",
              "      <td>112.105362</td>\n",
              "      <td>42.963554</td>\n",
              "      <td>108.540001</td>\n",
              "      <td>6.91</td>\n",
              "      <td>146.193253</td>\n",
              "      <td>10.761644</td>\n",
              "      <td>31.922256</td>\n",
              "      <td>45.220772</td>\n",
              "      <td>123.472305</td>\n",
              "      <td>52.990002</td>\n",
              "      <td>61.645340</td>\n",
              "      <td>79.692589</td>\n",
              "      <td>95.169441</td>\n",
              "      <td>93.750000</td>\n",
              "      <td>65.677902</td>\n",
              "      <td>32.932114</td>\n",
              "      <td>62.508064</td>\n",
              "      <td>804.059998</td>\n",
              "      <td>777.289978</td>\n",
              "      <td>47.520481</td>\n",
              "      <td>837.309998</td>\n",
              "      <td>9.51981</td>\n",
              "      <td>42.706928</td>\n",
              "      <td>35.288368</td>\n",
              "      <td>54.155262</td>\n",
              "      <td>59.305332</td>\n",
              "      <td>52.002068</td>\n",
              "      <td>103.212494</td>\n",
              "      <td>68.355148</td>\n",
              "      <td>88.066017</td>\n",
              "      <td>74.000618</td>\n",
              "      <td>46.267582</td>\n",
              "      <td>144.636215</td>\n",
              "      <td>31.016325</td>\n",
              "      <td>58.256134</td>\n",
              "      <td>92.610001</td>\n",
              "      <td>...</td>\n",
              "      <td>29.629341</td>\n",
              "      <td>42.178417</td>\n",
              "      <td>55.373524</td>\n",
              "      <td>78.239998</td>\n",
              "      <td>79.937149</td>\n",
              "      <td>41.633564</td>\n",
              "      <td>87.209999</td>\n",
              "      <td>46.460678</td>\n",
              "      <td>50.095135</td>\n",
              "      <td>37.567600</td>\n",
              "      <td>80.039871</td>\n",
              "      <td>64.433357</td>\n",
              "      <td>108.580971</td>\n",
              "      <td>34.721607</td>\n",
              "      <td>79.214218</td>\n",
              "      <td>64.965401</td>\n",
              "      <td>69.385086</td>\n",
              "      <td>88.244537</td>\n",
              "      <td>57.728222</td>\n",
              "      <td>158.490005</td>\n",
              "      <td>51.358322</td>\n",
              "      <td>37.978359</td>\n",
              "      <td>59.052296</td>\n",
              "      <td>72.783386</td>\n",
              "      <td>52.013783</td>\n",
              "      <td>17.428846</td>\n",
              "      <td>41.255169</td>\n",
              "      <td>26.899380</td>\n",
              "      <td>139.531525</td>\n",
              "      <td>23.011034</td>\n",
              "      <td>123.988655</td>\n",
              "      <td>89.726730</td>\n",
              "      <td>35.787510</td>\n",
              "      <td>50.331238</td>\n",
              "      <td>49.347263</td>\n",
              "      <td>59.752254</td>\n",
              "      <td>69.610001</td>\n",
              "      <td>125.281212</td>\n",
              "      <td>27.626471</td>\n",
              "      <td>50.397995</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>150.865356</td>\n",
              "      <td>38.848495</td>\n",
              "      <td>50.393078</td>\n",
              "      <td>127.800003</td>\n",
              "      <td>110.903267</td>\n",
              "      <td>42.769588</td>\n",
              "      <td>108.449997</td>\n",
              "      <td>6.95</td>\n",
              "      <td>144.056061</td>\n",
              "      <td>10.443401</td>\n",
              "      <td>31.789005</td>\n",
              "      <td>45.268795</td>\n",
              "      <td>124.799675</td>\n",
              "      <td>53.709999</td>\n",
              "      <td>62.796619</td>\n",
              "      <td>79.077354</td>\n",
              "      <td>93.017029</td>\n",
              "      <td>90.209999</td>\n",
              "      <td>64.972633</td>\n",
              "      <td>32.493702</td>\n",
              "      <td>61.803303</td>\n",
              "      <td>800.380005</td>\n",
              "      <td>772.559998</td>\n",
              "      <td>47.234901</td>\n",
              "      <td>836.739990</td>\n",
              "      <td>9.51981</td>\n",
              "      <td>42.229328</td>\n",
              "      <td>36.339024</td>\n",
              "      <td>53.564888</td>\n",
              "      <td>59.092339</td>\n",
              "      <td>51.633999</td>\n",
              "      <td>102.228912</td>\n",
              "      <td>67.642738</td>\n",
              "      <td>89.001686</td>\n",
              "      <td>73.395988</td>\n",
              "      <td>45.928669</td>\n",
              "      <td>145.095734</td>\n",
              "      <td>30.767885</td>\n",
              "      <td>58.075367</td>\n",
              "      <td>91.870003</td>\n",
              "      <td>...</td>\n",
              "      <td>29.453123</td>\n",
              "      <td>42.098831</td>\n",
              "      <td>54.244583</td>\n",
              "      <td>78.830002</td>\n",
              "      <td>79.887985</td>\n",
              "      <td>41.553467</td>\n",
              "      <td>86.790001</td>\n",
              "      <td>46.402641</td>\n",
              "      <td>50.269009</td>\n",
              "      <td>37.646442</td>\n",
              "      <td>80.194717</td>\n",
              "      <td>63.319260</td>\n",
              "      <td>105.688133</td>\n",
              "      <td>34.529243</td>\n",
              "      <td>78.738838</td>\n",
              "      <td>64.866287</td>\n",
              "      <td>69.178513</td>\n",
              "      <td>87.892921</td>\n",
              "      <td>57.456604</td>\n",
              "      <td>159.050003</td>\n",
              "      <td>50.629280</td>\n",
              "      <td>37.592400</td>\n",
              "      <td>58.278305</td>\n",
              "      <td>72.763847</td>\n",
              "      <td>52.360722</td>\n",
              "      <td>17.253054</td>\n",
              "      <td>41.025394</td>\n",
              "      <td>26.781473</td>\n",
              "      <td>139.454086</td>\n",
              "      <td>22.936153</td>\n",
              "      <td>123.372269</td>\n",
              "      <td>91.439835</td>\n",
              "      <td>35.300369</td>\n",
              "      <td>49.951485</td>\n",
              "      <td>49.366081</td>\n",
              "      <td>59.824631</td>\n",
              "      <td>69.919998</td>\n",
              "      <td>125.146309</td>\n",
              "      <td>27.706623</td>\n",
              "      <td>50.737137</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>147.969437</td>\n",
              "      <td>38.437649</td>\n",
              "      <td>50.209766</td>\n",
              "      <td>127.370003</td>\n",
              "      <td>108.499123</td>\n",
              "      <td>42.798687</td>\n",
              "      <td>108.389999</td>\n",
              "      <td>6.97</td>\n",
              "      <td>143.663879</td>\n",
              "      <td>10.007910</td>\n",
              "      <td>31.757912</td>\n",
              "      <td>44.971085</td>\n",
              "      <td>121.914253</td>\n",
              "      <td>54.639999</td>\n",
              "      <td>64.312950</td>\n",
              "      <td>77.017220</td>\n",
              "      <td>92.334579</td>\n",
              "      <td>89.459999</td>\n",
              "      <td>64.219681</td>\n",
              "      <td>31.745831</td>\n",
              "      <td>61.297310</td>\n",
              "      <td>802.789978</td>\n",
              "      <td>776.429993</td>\n",
              "      <td>46.430740</td>\n",
              "      <td>834.030029</td>\n",
              "      <td>9.51981</td>\n",
              "      <td>41.143837</td>\n",
              "      <td>36.271553</td>\n",
              "      <td>52.443146</td>\n",
              "      <td>59.184952</td>\n",
              "      <td>51.774216</td>\n",
              "      <td>100.735313</td>\n",
              "      <td>65.679054</td>\n",
              "      <td>89.178215</td>\n",
              "      <td>73.771591</td>\n",
              "      <td>45.638168</td>\n",
              "      <td>145.277786</td>\n",
              "      <td>30.720116</td>\n",
              "      <td>58.066330</td>\n",
              "      <td>91.739998</td>\n",
              "      <td>...</td>\n",
              "      <td>29.914646</td>\n",
              "      <td>42.695702</td>\n",
              "      <td>53.421379</td>\n",
              "      <td>78.040001</td>\n",
              "      <td>79.317566</td>\n",
              "      <td>41.056873</td>\n",
              "      <td>86.419998</td>\n",
              "      <td>46.576717</td>\n",
              "      <td>50.836399</td>\n",
              "      <td>37.419777</td>\n",
              "      <td>80.068909</td>\n",
              "      <td>62.663521</td>\n",
              "      <td>105.821800</td>\n",
              "      <td>34.607395</td>\n",
              "      <td>78.942566</td>\n",
              "      <td>64.632088</td>\n",
              "      <td>69.058044</td>\n",
              "      <td>87.987953</td>\n",
              "      <td>56.723221</td>\n",
              "      <td>158.199997</td>\n",
              "      <td>49.256981</td>\n",
              "      <td>37.523777</td>\n",
              "      <td>56.769817</td>\n",
              "      <td>71.562187</td>\n",
              "      <td>52.209488</td>\n",
              "      <td>17.018656</td>\n",
              "      <td>40.736069</td>\n",
              "      <td>26.132990</td>\n",
              "      <td>139.626190</td>\n",
              "      <td>22.621653</td>\n",
              "      <td>122.867989</td>\n",
              "      <td>88.768867</td>\n",
              "      <td>34.439171</td>\n",
              "      <td>49.145657</td>\n",
              "      <td>49.168491</td>\n",
              "      <td>59.403515</td>\n",
              "      <td>69.410004</td>\n",
              "      <td>124.519951</td>\n",
              "      <td>27.964897</td>\n",
              "      <td>50.688683</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>148.581345</td>\n",
              "      <td>39.021973</td>\n",
              "      <td>50.520592</td>\n",
              "      <td>127.449997</td>\n",
              "      <td>108.471596</td>\n",
              "      <td>43.186615</td>\n",
              "      <td>108.800003</td>\n",
              "      <td>6.78</td>\n",
              "      <td>146.006973</td>\n",
              "      <td>9.932537</td>\n",
              "      <td>31.935579</td>\n",
              "      <td>45.278381</td>\n",
              "      <td>121.612411</td>\n",
              "      <td>55.980000</td>\n",
              "      <td>63.938557</td>\n",
              "      <td>77.781601</td>\n",
              "      <td>90.505913</td>\n",
              "      <td>89.480003</td>\n",
              "      <td>64.381706</td>\n",
              "      <td>31.616882</td>\n",
              "      <td>61.270210</td>\n",
              "      <td>801.229980</td>\n",
              "      <td>776.469971</td>\n",
              "      <td>46.009869</td>\n",
              "      <td>844.359985</td>\n",
              "      <td>9.51981</td>\n",
              "      <td>41.430401</td>\n",
              "      <td>36.502880</td>\n",
              "      <td>52.443146</td>\n",
              "      <td>59.901596</td>\n",
              "      <td>52.492817</td>\n",
              "      <td>97.848305</td>\n",
              "      <td>65.003166</td>\n",
              "      <td>91.084839</td>\n",
              "      <td>73.313553</td>\n",
              "      <td>46.596817</td>\n",
              "      <td>145.009064</td>\n",
              "      <td>31.006773</td>\n",
              "      <td>58.464035</td>\n",
              "      <td>92.379997</td>\n",
              "      <td>...</td>\n",
              "      <td>30.552372</td>\n",
              "      <td>43.610882</td>\n",
              "      <td>52.864742</td>\n",
              "      <td>77.849998</td>\n",
              "      <td>78.973335</td>\n",
              "      <td>40.723114</td>\n",
              "      <td>87.650002</td>\n",
              "      <td>45.349922</td>\n",
              "      <td>51.623428</td>\n",
              "      <td>37.478905</td>\n",
              "      <td>80.668961</td>\n",
              "      <td>61.199276</td>\n",
              "      <td>104.399269</td>\n",
              "      <td>34.631435</td>\n",
              "      <td>79.146324</td>\n",
              "      <td>64.560013</td>\n",
              "      <td>69.290421</td>\n",
              "      <td>87.854912</td>\n",
              "      <td>56.505932</td>\n",
              "      <td>158.270004</td>\n",
              "      <td>49.145473</td>\n",
              "      <td>38.587318</td>\n",
              "      <td>55.585148</td>\n",
              "      <td>72.265602</td>\n",
              "      <td>52.698761</td>\n",
              "      <td>16.993544</td>\n",
              "      <td>39.604282</td>\n",
              "      <td>25.804541</td>\n",
              "      <td>141.200806</td>\n",
              "      <td>22.861273</td>\n",
              "      <td>123.232193</td>\n",
              "      <td>90.205650</td>\n",
              "      <td>34.369579</td>\n",
              "      <td>49.469841</td>\n",
              "      <td>48.876835</td>\n",
              "      <td>58.311253</td>\n",
              "      <td>70.000000</td>\n",
              "      <td>125.791908</td>\n",
              "      <td>28.338949</td>\n",
              "      <td>50.562714</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>147.934937</td>\n",
              "      <td>39.231968</td>\n",
              "      <td>50.074276</td>\n",
              "      <td>128.500000</td>\n",
              "      <td>108.756042</td>\n",
              "      <td>42.662914</td>\n",
              "      <td>108.559998</td>\n",
              "      <td>6.96</td>\n",
              "      <td>145.791321</td>\n",
              "      <td>9.765041</td>\n",
              "      <td>32.011089</td>\n",
              "      <td>45.422432</td>\n",
              "      <td>122.425407</td>\n",
              "      <td>54.860001</td>\n",
              "      <td>64.668640</td>\n",
              "      <td>77.968048</td>\n",
              "      <td>91.022133</td>\n",
              "      <td>89.440002</td>\n",
              "      <td>64.762932</td>\n",
              "      <td>31.539524</td>\n",
              "      <td>61.234070</td>\n",
              "      <td>803.080017</td>\n",
              "      <td>776.859985</td>\n",
              "      <td>46.430740</td>\n",
              "      <td>841.659973</td>\n",
              "      <td>9.51981</td>\n",
              "      <td>41.326202</td>\n",
              "      <td>36.792053</td>\n",
              "      <td>52.502171</td>\n",
              "      <td>57.649227</td>\n",
              "      <td>52.536633</td>\n",
              "      <td>98.822792</td>\n",
              "      <td>65.030571</td>\n",
              "      <td>90.749435</td>\n",
              "      <td>73.414307</td>\n",
              "      <td>46.664612</td>\n",
              "      <td>144.714203</td>\n",
              "      <td>31.302975</td>\n",
              "      <td>58.518276</td>\n",
              "      <td>92.110001</td>\n",
              "      <td>...</td>\n",
              "      <td>30.695026</td>\n",
              "      <td>43.690479</td>\n",
              "      <td>52.519772</td>\n",
              "      <td>77.279999</td>\n",
              "      <td>78.432442</td>\n",
              "      <td>40.715019</td>\n",
              "      <td>84.959999</td>\n",
              "      <td>45.325054</td>\n",
              "      <td>51.440403</td>\n",
              "      <td>36.306149</td>\n",
              "      <td>80.678635</td>\n",
              "      <td>61.434830</td>\n",
              "      <td>106.241890</td>\n",
              "      <td>34.655479</td>\n",
              "      <td>79.903030</td>\n",
              "      <td>62.479202</td>\n",
              "      <td>69.109680</td>\n",
              "      <td>88.216026</td>\n",
              "      <td>56.840923</td>\n",
              "      <td>157.759995</td>\n",
              "      <td>49.154060</td>\n",
              "      <td>38.750278</td>\n",
              "      <td>55.545658</td>\n",
              "      <td>73.183945</td>\n",
              "      <td>51.640160</td>\n",
              "      <td>17.077259</td>\n",
              "      <td>40.378658</td>\n",
              "      <td>26.191942</td>\n",
              "      <td>143.059372</td>\n",
              "      <td>22.846296</td>\n",
              "      <td>123.353615</td>\n",
              "      <td>90.021461</td>\n",
              "      <td>34.465267</td>\n",
              "      <td>49.488361</td>\n",
              "      <td>48.763931</td>\n",
              "      <td>57.534817</td>\n",
              "      <td>69.059998</td>\n",
              "      <td>126.023102</td>\n",
              "      <td>28.214266</td>\n",
              "      <td>50.049141</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 493 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "          MMM        ABT       ABBV  ...         ZBH       ZION        ZTS\n",
              "0  151.891006  38.611118  50.265553  ...  125.281212  27.626471  50.397995\n",
              "1  150.865356  38.848495  50.393078  ...  125.146309  27.706623  50.737137\n",
              "2  147.969437  38.437649  50.209766  ...  124.519951  27.964897  50.688683\n",
              "3  148.581345  39.021973  50.520592  ...  125.791908  28.338949  50.562714\n",
              "4  147.934937  39.231968  50.074276  ...  126.023102  28.214266  50.049141\n",
              "\n",
              "[5 rows x 493 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "01KifoK99e_u"
      },
      "source": [
        "debug_step = 0\n",
        "debug_corr_1 = None\n",
        "debug_corr_2 = None\n",
        "debug_corr_3 = None\n",
        "debug_corr_4 = None"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5jQMiNG3P3oU"
      },
      "source": [
        ""
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OoAOvHQ_EQmX"
      },
      "source": [
        "import math\n",
        "from torch import Tensor\n",
        "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
        "\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "\n",
        "    def __init__(self, d_model, dropout = 0.1, max_len = 1000):\n",
        "        super().__init__()\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "        position = torch.arange(max_len).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
        "        pe = torch.zeros(max_len, 1, d_model)\n",
        "        pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "    \n",
        "        x = x + self.pe[:x.size(0)]\n",
        "        return self.dropout(x)\n",
        "\n",
        "class TransformerModel(nn.Module):\n",
        "\n",
        "    def __init__(self, input_size, d_model, d_out, nhead, d_hid, nlayers, dropout=0.5):\n",
        "        super().__init__()\n",
        "\n",
        "        self.pos_encoder = PositionalEncoding(d_model, dropout)\n",
        "        encoder_layers = TransformerEncoderLayer(d_model, nhead, d_hid, dropout)\n",
        "        self.transformer_encoder = TransformerEncoder(encoder_layers, nlayers)\n",
        "        self.encoder = nn.Linear(input_size, d_model, bias=False)\n",
        "        self.d_model = d_model\n",
        "        self.decoder = nn.Linear(d_model, d_out)\n",
        "\n",
        "        self.init_weights()\n",
        "\n",
        "    def init_weights(self) -> None:\n",
        "        initrange = 0.1\n",
        "        self.encoder.weight.data.uniform_(-initrange, initrange)\n",
        "        self.decoder.bias.data.zero_()\n",
        "        self.decoder.weight.data.uniform_(-initrange, initrange)\n",
        "\n",
        "    def forward(self, src: Tensor) -> Tensor:\n",
        "        \n",
        "        src = self.encoder(src) * math.sqrt(self.d_model)\n",
        "        src = self.pos_encoder(src)\n",
        "        output = self.transformer_encoder(src)\n",
        "        output = self.decoder(output)\n",
        "\n",
        "\n",
        "        return output"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QHnLhbHeXTEn"
      },
      "source": [
        "# move training dependencies here for the ease of training\n",
        "\n",
        "\n",
        "from utils.math_utils import evaluate\n",
        "\n",
        "\n",
        "\n",
        "class GLU(nn.Module):\n",
        "    def __init__(self, input_channel, output_channel):\n",
        "        super(GLU, self).__init__()\n",
        "        self.linear_left = nn.Linear(input_channel, output_channel)\n",
        "        self.linear_right = nn.Linear(input_channel, output_channel)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return torch.mul(self.linear_left(x), torch.sigmoid(self.linear_right(x)))\n",
        "\n",
        "\n",
        "class StockBlockLayer(nn.Module):\n",
        "    def __init__(self, time_step, unit, multi_layer, stack_cnt=0):\n",
        "        super(StockBlockLayer, self).__init__()\n",
        "        self.time_step = time_step\n",
        "        self.unit = unit\n",
        "        self.stack_cnt = stack_cnt\n",
        "        self.multi = multi_layer\n",
        "        self.weight = nn.Parameter(\n",
        "            torch.Tensor(1, 3 + 1, 1, self.time_step * self.multi,\n",
        "                         self.multi * self.time_step))  # [K+1, 1, in_c, out_c]\n",
        "        nn.init.xavier_normal_(self.weight)\n",
        "        self.forecast = nn.Linear(self.time_step * self.multi, self.time_step * self.multi)\n",
        "        self.forecast_result = nn.Linear(self.time_step * self.multi, self.time_step)\n",
        "        if self.stack_cnt == 0:\n",
        "            self.backcast = nn.Linear(self.time_step * self.multi, self.time_step)\n",
        "        self.backcast_short_cut = nn.Linear(self.time_step, self.time_step)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.GLUs = nn.ModuleList()\n",
        "        self.output_channel = 4 * self.multi\n",
        "        for i in range(3):\n",
        "            if i == 0:\n",
        "                self.GLUs.append(GLU(self.time_step * 4, self.time_step * self.output_channel))\n",
        "                self.GLUs.append(GLU(self.time_step * 4, self.time_step * self.output_channel))\n",
        "            elif i == 1:\n",
        "                self.GLUs.append(GLU(self.time_step * self.output_channel, self.time_step * self.output_channel))\n",
        "                self.GLUs.append(GLU(self.time_step * self.output_channel, self.time_step * self.output_channel))\n",
        "            else:\n",
        "                self.GLUs.append(GLU(self.time_step * self.output_channel, self.time_step * self.output_channel))\n",
        "                self.GLUs.append(GLU(self.time_step * self.output_channel, self.time_step * self.output_channel))\n",
        "\n",
        "    def spe_seq_cell(self, input):\n",
        "        batch_size, k, input_channel, node_cnt, time_step = input.size()\n",
        "        input = input.view(batch_size, -1, node_cnt, time_step)\n",
        "        ffted = torch.rfft(input, 1, onesided=False)\n",
        "        real = ffted[..., 0].permute(0, 2, 1, 3).contiguous().reshape(batch_size, node_cnt, -1)\n",
        "        img = ffted[..., 1].permute(0, 2, 1, 3).contiguous().reshape(batch_size, node_cnt, -1)\n",
        "        for i in range(3):\n",
        "            real = self.GLUs[i * 2](real)\n",
        "            img = self.GLUs[2 * i + 1](img)\n",
        "        real = real.reshape(batch_size, node_cnt, 4, -1).permute(0, 2, 1, 3).contiguous()\n",
        "        img = img.reshape(batch_size, node_cnt, 4, -1).permute(0, 2, 1, 3).contiguous()\n",
        "        time_step_as_inner = torch.cat([real.unsqueeze(-1), img.unsqueeze(-1)], dim=-1)\n",
        "        iffted = torch.irfft(time_step_as_inner, 1, onesided=False)\n",
        "        return iffted\n",
        "\n",
        "    def forward(self, x, mul_L):\n",
        "        mul_L = mul_L.unsqueeze(1)\n",
        "        x = x.unsqueeze(1)\n",
        "        gfted = torch.matmul(mul_L, x)\n",
        "        gconv_input = self.spe_seq_cell(gfted).unsqueeze(2)\n",
        "        igfted = torch.matmul(gconv_input, self.weight)\n",
        "        igfted = torch.sum(igfted, dim=1)\n",
        "        forecast_source = torch.sigmoid(self.forecast(igfted).squeeze(1))\n",
        "        forecast = self.forecast_result(forecast_source)\n",
        "        if self.stack_cnt == 0:\n",
        "            backcast_short = self.backcast_short_cut(x).squeeze(1)\n",
        "            backcast_source = torch.sigmoid(self.backcast(igfted) - backcast_short)\n",
        "        else:\n",
        "            backcast_source = None\n",
        "        return forecast, backcast_source\n",
        "\n",
        "\n",
        "class Model(nn.Module):\n",
        "    def __init__(self, units, stack_cnt, time_step, multi_layer, horizon=1, dropout_rate=0.5, leaky_rate=0.2,\n",
        "                 device='cpu'):\n",
        "        super(Model, self).__init__()\n",
        "        self.unit = units\n",
        "        self.stack_cnt = stack_cnt\n",
        "        self.unit = units\n",
        "        self.alpha = leaky_rate\n",
        "        self.correlation_time_step = 4\n",
        "        self.time_step = time_step\n",
        "        self.horizon = horizon\n",
        "        self.weight_key = nn.Parameter(torch.zeros(size=(self.unit, 1)))\n",
        "        nn.init.xavier_uniform_(self.weight_key.data, gain=1.414)\n",
        "        self.weight_query = nn.Parameter(torch.zeros(size=(self.unit, 1)))\n",
        "        nn.init.xavier_uniform_(self.weight_query.data, gain=1.414)\n",
        "        #self.GRU = nn.GRU(self.time_step, self.unit)\n",
        "\n",
        "        self.transformer = TransformerModel(input_size=self.correlation_time_step, d_model=16, d_out=self.unit, nhead=1, d_hid=8, nlayers=8)\n",
        "        \n",
        "        self.multi_layer = multi_layer\n",
        "        self.stock_block = nn.ModuleList()\n",
        "        self.stock_block.extend(\n",
        "            [StockBlockLayer(self.time_step, self.unit, self.multi_layer, stack_cnt=i) for i in range(self.stack_cnt)])\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(int(self.time_step), int(self.time_step)),\n",
        "            nn.LeakyReLU(),\n",
        "            nn.Linear(int(self.time_step), self.horizon),\n",
        "        )\n",
        "        self.leakyrelu = nn.LeakyReLU(self.alpha)\n",
        "        self.dropout = nn.Dropout(p=dropout_rate)\n",
        "        self.to(device)\n",
        "\n",
        "    def get_laplacian(self, graph, normalize):\n",
        "        \"\"\"\n",
        "        return the laplacian of the graph.\n",
        "        :param graph: the graph structure without self loop, [N, N].\n",
        "        :param normalize: whether to used the normalized laplacian.\n",
        "        :return: graph laplacian.\n",
        "        \"\"\"\n",
        "        if normalize:\n",
        "            D = torch.diag(torch.sum(graph, dim=-1) ** (-1 / 2))\n",
        "            L = torch.eye(graph.size(0), device=graph.device, dtype=graph.dtype) - torch.mm(torch.mm(D, graph), D)\n",
        "        else:\n",
        "            D = torch.diag(torch.sum(graph, dim=-1))\n",
        "            L = D - graph\n",
        "        return L\n",
        "\n",
        "    def cheb_polynomial(self, laplacian):\n",
        "        \"\"\"\n",
        "        Compute the Chebyshev Polynomial, according to the graph laplacian.\n",
        "        :param laplacian: the graph laplacian, [N, N].\n",
        "        :return: the multi order Chebyshev laplacian, [K, N, N].\n",
        "        \"\"\"\n",
        "        N = laplacian.size(0)  # [N, N]\n",
        "        laplacian = laplacian.unsqueeze(0)\n",
        "        first_laplacian = torch.zeros([1, N, N], device=laplacian.device, dtype=torch.float)\n",
        "        second_laplacian = laplacian\n",
        "        third_laplacian = (2 * torch.matmul(laplacian, second_laplacian)) - first_laplacian\n",
        "        forth_laplacian = 2 * torch.matmul(laplacian, third_laplacian) - second_laplacian\n",
        "        multi_order_laplacian = torch.cat([first_laplacian, second_laplacian, third_laplacian, forth_laplacian], dim=0)\n",
        "        return multi_order_laplacian\n",
        "\n",
        "    def latent_correlation_layer(self, x):\n",
        "        global debug_step, debug_corr_3, debug_corr_4\n",
        "        \n",
        "        #input, _ = self.GRU(x.permute(2, 0, 1).contiguous())\n",
        "        #print(x.shape)\n",
        "        #print(x.permute(0, 2, 1).shape)\n",
        "        \n",
        "        input = self.transformer(x.permute(0, 2, 1).contiguous())\n",
        "        #input = input.permute(1, 0, 2)\n",
        "        #input = input.permute(1, 0, 2).contiguous()\n",
        "        attention = self.self_graph_attention(input)\n",
        "        attention = torch.mean(attention, dim=0)\n",
        "        if debug_step == 49:\n",
        "            debug_corr_3 = attention.cpu().detach().numpy()\n",
        "        degree = torch.sum(attention, dim=1)\n",
        "        # laplacian is sym or not\n",
        "        attention = 0.5 * (attention + attention.T)\n",
        "        if debug_step == 49:\n",
        "            debug_corr_4 = attention.cpu().detach().numpy()\n",
        "        degree_l = torch.diag(degree)\n",
        "        diagonal_degree_hat = torch.diag(1 / (torch.sqrt(degree) + 1e-7))\n",
        "        laplacian = torch.matmul(diagonal_degree_hat,\n",
        "                                 torch.matmul(degree_l - attention, diagonal_degree_hat))\n",
        "        mul_L = self.cheb_polynomial(laplacian)\n",
        "        return mul_L, attention\n",
        "\n",
        "    def self_graph_attention(self, input):\n",
        "        global debug_step, debug_corr_1, debug_corr_2\n",
        "\n",
        "        input = input.permute(0, 2, 1).contiguous()\n",
        "        bat, N, fea = input.size()\n",
        "        key = torch.matmul(input, self.weight_key)\n",
        "        query = torch.matmul(input, self.weight_query)\n",
        "        data = key.repeat(1, 1, N).view(bat, N * N, 1) + query.repeat(1, N, 1)\n",
        "        data = data.squeeze(2)\n",
        "        data = data.view(bat, N, -1)\n",
        "        if debug_step == 49:\n",
        "            debug_corr_1 = data.cpu().detach().numpy()\n",
        "        data = self.leakyrelu(data)\n",
        "        if debug_step == 49:\n",
        "            debug_corr_2 = data.cpu().detach().numpy()\n",
        "        debug_step += 1\n",
        "        attention = F.softmax(data, dim=2)\n",
        "        attention = self.dropout(attention)\n",
        "        return attention\n",
        "\n",
        "    def graph_fft(self, input, eigenvectors):\n",
        "        return torch.matmul(eigenvectors, input)\n",
        "\n",
        "    def forward(self, x):\n",
        "        mul_L, attention = self.latent_correlation_layer(x[:,-4:,:])\n",
        "        X = x.unsqueeze(1).permute(0, 1, 3, 2).contiguous()\n",
        "        result = []\n",
        "        for stack_i in range(self.stack_cnt):\n",
        "            forecast, X = self.stock_block[stack_i](X, mul_L)\n",
        "            result.append(forecast)\n",
        "        forecast = result[0] + result[1]\n",
        "        forecast = self.fc(forecast)\n",
        "        if forecast.size()[-1] == 1:\n",
        "            return forecast.unsqueeze(1).squeeze(-1), attention\n",
        "        else:\n",
        "            return forecast.permute(0, 2, 1).contiguous(), attention\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def train(train_data, valid_data, args, result_file):\n",
        "    node_cnt = train_data.shape[1]\n",
        "    model = Model(node_cnt, 2, args.window_size, args.multi_layer, horizon=args.horizon)\n",
        "    model.to(args.device)\n",
        "    if len(train_data) == 0:\n",
        "        raise Exception('Cannot organize enough training data')\n",
        "    if len(valid_data) == 0:\n",
        "        raise Exception('Cannot organize enough validation data')\n",
        "\n",
        "    if args.norm_method == 'z_score':\n",
        "        train_mean = np.mean(train_data, axis=0)\n",
        "        train_std = np.std(train_data, axis=0)\n",
        "        normalize_statistic = {\"mean\": train_mean.tolist(), \"std\": train_std.tolist()}\n",
        "    elif args.norm_method == 'min_max':\n",
        "        train_min = np.min(train_data, axis=0)\n",
        "        train_max = np.max(train_data, axis=0)\n",
        "        normalize_statistic = {\"min\": train_min.tolist(), \"max\": train_max.tolist()}\n",
        "    else:\n",
        "        normalize_statistic = None\n",
        "    if normalize_statistic is not None:\n",
        "        with open(os.path.join(result_file, 'norm_stat.json'), 'w') as f:\n",
        "            json.dump(normalize_statistic, f)\n",
        "\n",
        "    if args.optimizer == 'RMSProp':\n",
        "        my_optim = torch.optim.RMSprop(params=model.parameters(), lr=args.lr, eps=1e-08)\n",
        "    else:\n",
        "        my_optim = torch.optim.Adam(params=model.parameters(), lr=args.lr, betas=(0.9, 0.999))\n",
        "    my_lr_scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer=my_optim, gamma=args.decay_rate)\n",
        "\n",
        "    train_set = ForecastDataset(train_data, window_size=args.window_size, horizon=args.horizon,\n",
        "                                normalize_method=args.norm_method, norm_statistic=normalize_statistic)\n",
        "    valid_set = ForecastDataset(valid_data, window_size=args.window_size, horizon=args.horizon,\n",
        "                                normalize_method=args.norm_method, norm_statistic=normalize_statistic)\n",
        "    train_loader = torch.utils.data.DataLoader(train_set, batch_size=args.batch_size, drop_last=False, shuffle=True,\n",
        "                                         num_workers=0)\n",
        "    valid_loader = torch.utils.data.DataLoader(valid_set, batch_size=args.batch_size, shuffle=False, num_workers=0)\n",
        "\n",
        "    forecast_loss = nn.MSELoss(reduction='mean').to(args.device)\n",
        "\n",
        "    total_params = 0\n",
        "    for name, parameter in model.named_parameters():\n",
        "        if not parameter.requires_grad: continue\n",
        "        param = parameter.numel()\n",
        "        total_params += param\n",
        "    print(f\"Total Trainable Params: {total_params}\")\n",
        "\n",
        "    best_validate_rmse = np.inf\n",
        "    best_performance = None\n",
        "    validate_score_non_decrease_count = 0\n",
        "    performance_metrics = {}\n",
        "\n",
        "    training_losses = []\n",
        "\n",
        "    for epoch in range(args.epoch):\n",
        "        epoch_start_time = time.time()\n",
        "        model.train()\n",
        "        loss_total = 0\n",
        "        cnt = 0\n",
        "        for i, (inputs, target) in enumerate(train_loader):\n",
        "            inputs = inputs.to(args.device)\n",
        "            target = target.to(args.device)\n",
        "            model.zero_grad()\n",
        "            forecast, _ = model(inputs)\n",
        "            loss = forecast_loss(forecast, target)\n",
        "            cnt += 1\n",
        "            loss.backward()\n",
        "            my_optim.step()\n",
        "            loss_total += float(loss)\n",
        "        print('| end of epoch {:3d} | time: {:5.2f}s | train_total_loss {:5.4f}'.format(epoch, (\n",
        "                time.time() - epoch_start_time), loss_total / cnt))\n",
        "        training_losses.append(loss_total / cnt)\n",
        "        #save_model(model, result_file, epoch)\n",
        "        if (epoch+1) % args.exponential_decay_step == 0:\n",
        "            my_lr_scheduler.step()\n",
        "        if (epoch + 1) % args.validate_freq == 0:\n",
        "            is_best_for_now = False\n",
        "            print('------ validate on data: VALIDATE ------')\n",
        "            performance_metrics = \\\n",
        "                validate(model, valid_loader, args.device, args.norm_method, normalize_statistic,\n",
        "                         node_cnt, args.window_size, args.horizon,\n",
        "                         result_file=result_file)\n",
        "            if best_validate_rmse > performance_metrics['rmse_norm']:\n",
        "                best_validate_rmse = performance_metrics['rmse_norm']\n",
        "                is_best_for_now = True\n",
        "                validate_score_non_decrease_count = 0\n",
        "\n",
        "                #added\n",
        "                best_performance = performance_metrics\n",
        "            else:\n",
        "                validate_score_non_decrease_count += 1\n",
        "            # save model\n",
        "            if is_best_for_now:\n",
        "                save_model(model, result_file)\n",
        "        # early stop\n",
        "        if args.early_stop and validate_score_non_decrease_count >= args.early_stop_step:\n",
        "            break\n",
        "    return performance_metrics, normalize_statistic, training_losses, best_performance \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def save_model(model, model_dir, epoch=None):\n",
        "    if model_dir is None:\n",
        "        return\n",
        "    if not os.path.exists(model_dir):\n",
        "        os.makedirs(model_dir)\n",
        "    epoch = str(epoch) if epoch else ''\n",
        "    file_name = os.path.join(model_dir, epoch + '_stemgnn.pt')\n",
        "    with open(file_name, 'wb') as f:\n",
        "        torch.save(model, f)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def normalized(data, normalize_method, norm_statistic=None):\n",
        "    if normalize_method == 'min_max':\n",
        "        if not norm_statistic:\n",
        "            norm_statistic = dict(max=np.max(data, axis=0), min=np.min(data, axis=0))\n",
        "        scale = norm_statistic['max'] - norm_statistic['min'] + 1e-5\n",
        "        data = (data - norm_statistic['min']) / scale\n",
        "        data = np.clip(data, 0.0, 1.0)\n",
        "    elif normalize_method == 'z_score':\n",
        "        if not norm_statistic:\n",
        "            norm_statistic = dict(mean=np.mean(data, axis=0), std=np.std(data, axis=0))\n",
        "        mean = norm_statistic['mean']\n",
        "        std = norm_statistic['std']\n",
        "        std = [1 if i == 0 else i for i in std]\n",
        "        data = (data - mean) / std\n",
        "        norm_statistic['std'] = std\n",
        "    return data, norm_statistic\n",
        "\n",
        "\n",
        "def de_normalized(data, normalize_method, norm_statistic):\n",
        "    if normalize_method == 'min_max':\n",
        "        if not norm_statistic:\n",
        "            norm_statistic = dict(max=np.max(data, axis=0), min=np.min(data, axis=0))\n",
        "        scale = norm_statistic['max'] - norm_statistic['min'] + 1e-8\n",
        "        data = data * scale + norm_statistic['min']\n",
        "    elif normalize_method == 'z_score':\n",
        "        if not norm_statistic:\n",
        "            norm_statistic = dict(mean=np.mean(data, axis=0), std=np.std(data, axis=0))\n",
        "        mean = norm_statistic['mean']\n",
        "        std = norm_statistic['std']\n",
        "        std = [1 if i == 0 else i for i in std]\n",
        "        data = data * std + mean\n",
        "    return data\n",
        "\n",
        "\n",
        "class ForecastDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, df, window_size, horizon, normalize_method=None, norm_statistic=None, interval=1):\n",
        "        self.window_size = window_size\n",
        "        self.interval = interval\n",
        "        self.horizon = horizon\n",
        "        self.normalize_method = normalize_method\n",
        "        self.norm_statistic = norm_statistic\n",
        "        df = pd.DataFrame(df)\n",
        "        df = df.fillna(method='ffill', limit=len(df)).fillna(method='bfill', limit=len(df)).values\n",
        "        self.data = df\n",
        "        self.df_length = len(df)\n",
        "        self.x_end_idx = self.get_x_end_idx()\n",
        "        if normalize_method:\n",
        "            self.data, _ = normalized(self.data, normalize_method, norm_statistic)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        hi = self.x_end_idx[index]\n",
        "        lo = hi - self.window_size\n",
        "        train_data = self.data[lo: hi]\n",
        "        target_data = self.data[hi:hi + self.horizon]\n",
        "        x = torch.from_numpy(train_data).type(torch.float)\n",
        "        y = torch.from_numpy(target_data).type(torch.float)\n",
        "        return x, y\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.x_end_idx)\n",
        "\n",
        "    def get_x_end_idx(self):\n",
        "        # each element `hi` in `x_index_set` is an upper bound for get training data\n",
        "        # training data range: [lo, hi), lo = hi - window_size\n",
        "        x_index_set = range(self.window_size, self.df_length - self.horizon + 1)\n",
        "        x_end_idx = [x_index_set[j * self.interval] for j in range((len(x_index_set)) // self.interval)]\n",
        "        return x_end_idx\n",
        "\n",
        "\n",
        "\n",
        "def validate(model, dataloader, device, normalize_method, statistic,\n",
        "             node_cnt, window_size, horizon,\n",
        "             result_file=None):\n",
        "    start = datetime.now()\n",
        "    forecast_norm, target_norm = inference(model, dataloader, device,\n",
        "                                           node_cnt, window_size, horizon)\n",
        "    if normalize_method and statistic:\n",
        "        forecast = de_normalized(forecast_norm, normalize_method, statistic)\n",
        "        target = de_normalized(target_norm, normalize_method, statistic)\n",
        "    else:\n",
        "        forecast, target = forecast_norm, target_norm\n",
        "    score = evaluate(target, forecast)\n",
        "    score_by_node = evaluate(target, forecast, by_node=True)\n",
        "    end = datetime.now()\n",
        "\n",
        "    score_norm = evaluate(target_norm, forecast_norm)\n",
        "    print(f'NORM: MAPE {score_norm[0]:7.9%}; MAE {score_norm[1]:7.9f}; RMSE {score_norm[2]:7.9f}.')\n",
        "    print(f'RAW : MAPE {score[0]:7.9%}; MAE {score[1]:7.9f}; RMSE {score[2]:7.9f}.')\n",
        "    if result_file:\n",
        "        if not os.path.exists(result_file):\n",
        "            os.makedirs(result_file)\n",
        "        step_to_print = 0\n",
        "        forcasting_2d = forecast[:, step_to_print, :]\n",
        "        forcasting_2d_target = target[:, step_to_print, :]\n",
        "\n",
        "        np.savetxt(f'{result_file}/target.csv', forcasting_2d_target, delimiter=\",\")\n",
        "        np.savetxt(f'{result_file}/predict.csv', forcasting_2d, delimiter=\",\")\n",
        "        np.savetxt(f'{result_file}/predict_abs_error.csv',\n",
        "                   np.abs(forcasting_2d - forcasting_2d_target), delimiter=\",\")\n",
        "        np.savetxt(f'{result_file}/predict_ape.csv',\n",
        "                   np.abs((forcasting_2d - forcasting_2d_target) / forcasting_2d_target), delimiter=\",\")\n",
        "\n",
        "    return dict(mae=score[1], mae_node=score_by_node[1], mape=score[0], mape_node=score_by_node[0],\n",
        "                rmse=score[2], rmse_node=score_by_node[2], mae_norm=score_norm[1], mape_norm=score_norm[0], rmse_norm=score_norm[2])\n",
        "\n",
        "\n",
        "\n",
        "def inference(model, dataloader, device, node_cnt, window_size, horizon):\n",
        "    forecast_set = []\n",
        "    target_set = []\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for i, (inputs, target) in enumerate(dataloader):\n",
        "            inputs = inputs.to(device)\n",
        "            target = target.to(device)\n",
        "            step = 0\n",
        "            forecast_steps = np.zeros([inputs.size()[0], horizon, node_cnt], dtype=np.float)\n",
        "            while step < horizon:\n",
        "                forecast_result, a = model(inputs)\n",
        "                len_model_output = forecast_result.size()[1]\n",
        "                if len_model_output == 0:\n",
        "                    raise Exception('Get blank inference result')\n",
        "                inputs[:, :window_size - len_model_output, :] = inputs[:, len_model_output:window_size,\n",
        "                                                                   :].clone()\n",
        "                inputs[:, window_size - len_model_output:, :] = forecast_result.clone()\n",
        "                forecast_steps[:, step:min(horizon - step, len_model_output) + step, :] = \\\n",
        "                    forecast_result[:, :min(horizon - step, len_model_output), :].detach().cpu().numpy()\n",
        "                step += min(horizon - step, len_model_output)\n",
        "            forecast_set.append(forecast_steps)\n",
        "            target_set.append(target.detach().cpu().numpy())\n",
        "    return np.concatenate(forecast_set, axis=0), np.concatenate(target_set, axis=0)"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q9uOk7rPvdGV"
      },
      "source": [
        "\n",
        "# def train(train_data, valid_data, args, result_file):\n",
        "#     node_cnt = train_data.shape[1]\n",
        "#     model = Model(node_cnt, 2, args.window_size, args.multi_layer, horizon=args.horizon)\n",
        "#     model.to(args.device)\n",
        "#     if len(train_data) == 0:\n",
        "#         raise Exception('Cannot organize enough training data')\n",
        "#     if len(valid_data) == 0:\n",
        "#         raise Exception('Cannot organize enough validation data')\n",
        "\n",
        "#     if args.norm_method == 'z_score':\n",
        "#         train_mean = np.mean(train_data, axis=0)\n",
        "#         train_std = np.std(train_data, axis=0)\n",
        "#         normalize_statistic = {\"mean\": train_mean.tolist(), \"std\": train_std.tolist()}\n",
        "#     elif args.norm_method == 'min_max':\n",
        "#         train_min = np.min(train_data, axis=0)\n",
        "#         train_max = np.max(train_data, axis=0)\n",
        "#         normalize_statistic = {\"min\": train_min.tolist(), \"max\": train_max.tolist()}\n",
        "#     else:\n",
        "#         normalize_statistic = None\n",
        "#     if normalize_statistic is not None:\n",
        "#         with open(os.path.join(result_file, 'norm_stat.json'), 'w') as f:\n",
        "#             json.dump(normalize_statistic, f)\n",
        "\n",
        "#     if args.optimizer == 'RMSProp':\n",
        "#         my_optim = torch.optim.RMSprop(params=model.parameters(), lr=args.lr, eps=1e-08)\n",
        "#     else:\n",
        "#         my_optim = torch.optim.Adam(params=model.parameters(), lr=args.lr, betas=(0.9, 0.999))\n",
        "#     my_lr_scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer=my_optim, gamma=args.decay_rate)\n",
        "\n",
        "#     train_set = ForecastDataset(train_data, window_size=args.window_size, horizon=args.horizon,\n",
        "#                                 normalize_method=args.norm_method, norm_statistic=normalize_statistic)\n",
        "#     valid_set = ForecastDataset(valid_data, window_size=args.window_size, horizon=args.horizon,\n",
        "#                                 normalize_method=args.norm_method, norm_statistic=normalize_statistic)\n",
        "#     train_loader = torch.utils.data.DataLoader(train_set, batch_size=args.batch_size, drop_last=False, shuffle=True,\n",
        "#                                          num_workers=0)\n",
        "#     valid_loader = torch.utils.data.DataLoader(valid_set, batch_size=args.batch_size, shuffle=False, num_workers=0)\n",
        "\n",
        "#     forecast_loss = nn.MSELoss(reduction='mean').to(args.device)\n",
        "\n",
        "#     total_params = 0\n",
        "#     for name, parameter in model.named_parameters():\n",
        "#         if not parameter.requires_grad: continue\n",
        "#         param = parameter.numel()\n",
        "#         total_params += param\n",
        "#     print(f\"Total Trainable Params: {total_params}\")\n",
        "\n",
        "#     return model, train_loader\n",
        "\n",
        "\n",
        "\n",
        "# net, train_loader = train(train_data, valid_data, args, result_train_file)\n"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l2ahzwOmwyMN"
      },
      "source": [
        "# train_set = ForecastDataset(train_data, window_size=args.window_size, horizon=args.horizon,\n",
        "#                                 normalize_method=args.norm_method, norm_statistic=None)\n",
        "# for it, batch in enumerate(train_loader):\n",
        "#   break"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-IkcRyvo6SqW",
        "outputId": "ec16a6cc-222d-4dc2-c14e-2610fd6f008b"
      },
      "source": [
        "args"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'batch_size': 8,\n",
              " 'dataset': 'sp500_prices_adjclose_30_09_16_30_09_21',\n",
              " 'decay_rate': 0.5,\n",
              " 'device': 'cuda',\n",
              " 'dropout_rate': 0.5,\n",
              " 'early_stop': False,\n",
              " 'epoch': 50,\n",
              " 'evaluate': True,\n",
              " 'exponential_decay_step': 5,\n",
              " 'horizon': 3,\n",
              " 'leakyrelu_rate': 0.2,\n",
              " 'lr': 0.0001,\n",
              " 'multi_layer': 5,\n",
              " 'norm_method': 'z_score',\n",
              " 'optimizer': 'RMSProp',\n",
              " 'test_length': 1,\n",
              " 'train': True,\n",
              " 'train_length': 7,\n",
              " 'valid_length': 2,\n",
              " 'validate_freq': 1,\n",
              " 'window_size': 12}"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gd-LLhBbzCfr"
      },
      "source": [
        "# def latent_correlation_layer(self, x):\n",
        "#       input, _ = self.GRU(x.permute(2, 0, 1).contiguous())\n",
        "#       input = input.permute(1, 0, 2).contiguous()\n",
        "#       attention = self.self_graph_attention(input)\n",
        "#       attention = torch.mean(attention, dim=0)\n",
        "#       degree = torch.sum(attention, dim=1)\n",
        "#       # laplacian is sym or not\n",
        "#       attention = 0.5 * (attention + attention.T)\n",
        "#       degree_l = torch.diag(degree)\n",
        "#       diagonal_degree_hat = torch.diag(1 / (torch.sqrt(degree) + 1e-7))\n",
        "#       laplacian = torch.matmul(diagonal_degree_hat,\n",
        "#                                 torch.matmul(degree_l - attention, diagonal_degree_hat))\n",
        "#       mul_L = self.cheb_polynomial(laplacian)\n",
        "#       return mul_L, attention\n",
        "\n",
        "\n",
        "# def self_graph_attention( input, weights):\n",
        "#     weight_key = weights[0]\n",
        "#     weight_query = weights[1]\n",
        "#     input = input.permute(0, 2, 1).contiguous()\n",
        "#     bat, N, fea = input.size()\n",
        "#     key = torch.matmul(input, weight_key)\n",
        "#     query = torch.matmul(input, weight_query)\n",
        "    \n",
        "#     data = key.repeat(1, 1, N).view(bat, N * N, 1) + query.repeat(1, N, 1)\n",
        "#     data = data.squeeze(2)\n",
        "#     data = data.view(bat, N, -1)\n",
        "#     # data = torch.nn.Leakyrelu(data)\n",
        "#     import pdb; pdb.set_trace()\n",
        "#     attention = F.softmax(data, dim=2)\n",
        "#     attention = nn.functional.dropout(attention)\n",
        "#     return attention"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CUJIVZnOFeRf",
        "outputId": "ac862880-db87-4a48-c4c6-61cd746e9a94"
      },
      "source": [
        "args"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'batch_size': 8,\n",
              " 'dataset': 'sp500_prices_adjclose_30_09_16_30_09_21',\n",
              " 'decay_rate': 0.5,\n",
              " 'device': 'cuda',\n",
              " 'dropout_rate': 0.5,\n",
              " 'early_stop': False,\n",
              " 'epoch': 50,\n",
              " 'evaluate': True,\n",
              " 'exponential_decay_step': 5,\n",
              " 'horizon': 3,\n",
              " 'leakyrelu_rate': 0.2,\n",
              " 'lr': 0.0001,\n",
              " 'multi_layer': 5,\n",
              " 'norm_method': 'z_score',\n",
              " 'optimizer': 'RMSProp',\n",
              " 'test_length': 1,\n",
              " 'train': True,\n",
              " 'train_length': 7,\n",
              " 'valid_length': 2,\n",
              " 'validate_freq': 1,\n",
              " 'window_size': 12}"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OQXaiXaR0KlJ"
      },
      "source": [
        "# mul_L, attention = net.latent_correlation_layer(batch[0])a\n",
        "# weight_key = nn.Parameter(torch.zeros(size=(228, 1)))\n",
        "# weight_query = nn.Parameter(torch.zeros(size=(228, 1)))\n",
        "\n",
        "# # self_graph_attention(input_, [weight_key, weight_query])\n",
        "# mul_L.shape"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wH8OWHpgC4sV"
      },
      "source": [
        ""
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "wcvDH6SRVuJ7",
        "outputId": "8fab1848-5b4d-4b59-ebfa-77f5cbed9486"
      },
      "source": [
        "'''\n",
        "before_train = datetime.now().timestamp()\n",
        "_, normalize_statistic, training_losses, best_performance = train(train_data, valid_data, args, result_train_file)\n",
        "after_train = datetime.now().timestamp()\n",
        "print(f'Training took {(after_train - before_train) / 60} minutes')\n",
        "print(\"Best performance\", best_performance)\n",
        "'''"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\nbefore_train = datetime.now().timestamp()\\n_, normalize_statistic, training_losses, best_performance = train(train_data, valid_data, args, result_train_file)\\nafter_train = datetime.now().timestamp()\\nprint(f\\'Training took {(after_train - before_train) / 60} minutes\\')\\nprint(\"Best performance\", best_performance)\\n'"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PB-RP5xhCiU_"
      },
      "source": [
        "# plot training_losses\n",
        "def plot_loss(losses):\n",
        "    import matplotlib.pyplot as plt\n",
        "\n",
        "    plt.plot(range(50), losses)\n",
        "    plt.show()"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZHuYjOVdKGXt"
      },
      "source": [
        "#plot_loss(training_losses)"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "go5g7agHV8KD"
      },
      "source": [
        "#prediction = pd.read_csv('output/sp500_prices_adjclose_30_09_16_30_09_21/train/predict.csv',header=None)\n",
        "#target = pd.read_csv('output/sp500_prices_adjclose_30_09_16_30_09_21/train/target.csv',header=None)"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mQq_-MvoBnbu"
      },
      "source": [
        "#print(prediction.shape)\n",
        "#print(target.shape)"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xs6nBFnLaiXx",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "d6647377-a7a9-448d-df9a-4570692b9552"
      },
      "source": [
        "'''\n",
        "target_data = pd.read_csv(data_file).set_index('date').iloc[(train_data.shape[0] + 12):((train_data.shape[0] + 12) + prediction.shape[0]),]\n",
        "prediction.columns = np.array(target_data.columns) + 'pred'\n",
        "prediction['date'] = target_data.index\n",
        "prediction = prediction.set_index('date')\n",
        "'''"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"\\ntarget_data = pd.read_csv(data_file).set_index('date').iloc[(train_data.shape[0] + 12):((train_data.shape[0] + 12) + prediction.shape[0]),]\\nprediction.columns = np.array(target_data.columns) + 'pred'\\nprediction['date'] = target_data.index\\nprediction = prediction.set_index('date')\\n\""
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w9iOnxsqcSTi",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "outputId": "a4d2131a-eee7-478d-a7a4-b3832b2c2649"
      },
      "source": [
        "'''\n",
        "_, ax = plt.subplots(1,2,figsize=(20,10))\n",
        "ax[0].set_title('Target')\n",
        "target_data.iloc[:,:10].plot(ax=ax[0])\n",
        "for line, name in zip(ax[0].lines, target_data.columns):\n",
        "    y = line.get_ydata()[-1]\n",
        "    ax[0].annotate(name, xy=(1,y), xytext=(6,0), color=line.get_color(), \n",
        "                xycoords = ax[0].get_yaxis_transform(), textcoords=\"offset points\",\n",
        "                size=14, va=\"center\")\n",
        "\n",
        "\n",
        "# _, ax = plt.subplots(figsize=(10,10))\n",
        "prediction.iloc[:,:10].plot(ax=ax[1])\n",
        "ax[1].set_title('Prediction')\n",
        "for line, name in zip(ax[1].lines, prediction.columns):\n",
        "    y = line.get_ydata()[-1]\n",
        "    ax[1].annotate(name, xy=(1,y), xytext=(6,0), color=line.get_color(), \n",
        "                xycoords = ax[1].get_yaxis_transform(), textcoords=\"offset points\",\n",
        "                size=14, va=\"center\")\n",
        "ax[1].set_ylim(0,550)\n",
        "'''"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\n_, ax = plt.subplots(1,2,figsize=(20,10))\\nax[0].set_title(\\'Target\\')\\ntarget_data.iloc[:,:10].plot(ax=ax[0])\\nfor line, name in zip(ax[0].lines, target_data.columns):\\n    y = line.get_ydata()[-1]\\n    ax[0].annotate(name, xy=(1,y), xytext=(6,0), color=line.get_color(), \\n                xycoords = ax[0].get_yaxis_transform(), textcoords=\"offset points\",\\n                size=14, va=\"center\")\\n\\n\\n# _, ax = plt.subplots(figsize=(10,10))\\nprediction.iloc[:,:10].plot(ax=ax[1])\\nax[1].set_title(\\'Prediction\\')\\nfor line, name in zip(ax[1].lines, prediction.columns):\\n    y = line.get_ydata()[-1]\\n    ax[1].annotate(name, xy=(1,y), xytext=(6,0), color=line.get_color(), \\n                xycoords = ax[1].get_yaxis_transform(), textcoords=\"offset points\",\\n                size=14, va=\"center\")\\nax[1].set_ylim(0,550)\\n'"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "8XusvW6pOX9r",
        "outputId": "05f87e8f-cad6-4b4a-d6cf-a2ea0905357f"
      },
      "source": [
        "# partition dataset into the three clusters provided by Suhwan\n",
        "# path of the clusters csv is /content/drive/MyDrive/Nanyang/class/CE7454/cluster.csv\n",
        "\n",
        "clusters_partition = pd.read_csv(cluster_path)\n",
        "clusters = range(1, 5)\n",
        "clusters_partition.head()"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ts1</th>\n",
              "      <th>cluster</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>A</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>AAL</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>AAP</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>AAPL</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>ABBV</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    ts1  cluster\n",
              "0     A        1\n",
              "1   AAL        2\n",
              "2   AAP        1\n",
              "3  AAPL        1\n",
              "4  ABBV        1"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "632r_d-GTLQm"
      },
      "source": [
        "'''\n",
        "full_data = pd.read_csv(data_file).drop('date',1)\n",
        "result_train_files = []\n",
        "result_test_files = []\n",
        "\n",
        "for cluster in clusters:\n",
        "    result_train_file = os.path.join('output', str(cluster), 'train')\n",
        "    result_test_file = os.path.join('output', str(cluster), 'test')\n",
        "    if not os.path.exists(result_train_file):\n",
        "        os.makedirs(result_train_file)\n",
        "    if not os.path.exists(result_test_file):\n",
        "        os.makedirs(result_test_file)\n",
        "    result_train_files.append(result_train_file)\n",
        "    result_test_files.append(result_test_file)\n",
        "'''\n",
        "\n",
        "\n",
        "full_data = pd.read_csv(data_file).drop('date',1)\n",
        "# create files\n",
        "result_train_file = os.path.join('output', str(cluster), 'train')\n",
        "result_test_file = os.path.join('output', str(cluster), 'test')\n",
        "if not os.path.exists(result_train_file):\n",
        "    os.makedirs(result_train_file)\n",
        "if not os.path.exists(result_test_file):\n",
        "    os.makedirs(result_test_file)"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4rDk6I7gWzCE",
        "outputId": "03780d30-0681-4abd-ef34-579393ce1e0d"
      },
      "source": [
        "# 400 in cluster 1, 20 in cluster 2, 71 in cluster 3\n",
        "'''\n",
        "train_data = []\n",
        "valid_data = []\n",
        "test_data = []\n",
        "\n",
        "for cluster in clusters:\n",
        "    print(\"Cluster\", cluster)\n",
        "    selected_stocks = clusters_partition.loc[:, 'ts1'][clusters_partition[\"cluster\"] == cluster]\n",
        "    print(selected_stocks.tolist())\n",
        "    cluster_data = full_data[selected_stocks].values\n",
        "    train_data.append(cluster_data[:int(train_ratio * len(data))])\n",
        "    valid_data.append(cluster_data[int(train_ratio * len(data)):int((train_ratio + valid_ratio) * len(data))])\n",
        "    test_data.append(cluster_data[int((train_ratio + valid_ratio) * len(data)):])\n",
        "'''\n",
        "\n",
        "selected_stocks = clusters_partition.loc[:, 'ts1'][clusters_partition[\"cluster\"] == cluster]\n",
        "print(selected_stocks.tolist())\n",
        "cluster_data = full_data[selected_stocks].values\n",
        "train_ratio = args.train_length / (args.train_length + args.valid_length + args.test_length)\n",
        "valid_ratio = args.valid_length / (args.train_length + args.valid_length + args.test_length)\n",
        "train_data = cluster_data[:int(train_ratio * len(cluster_data))]\n",
        "valid_data = cluster_data[int(train_ratio * len(cluster_data)):int((train_ratio + valid_ratio) * len(cluster_data))]\n",
        "test_data = cluster_data[int((train_ratio + valid_ratio) * len(cluster_data)):]"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['A', 'AAP', 'AAPL', 'ABBV', 'ABC', 'ABMD', 'ABT', 'ACN', 'ADBE', 'ADI', 'ADM', 'ADP', 'ADSK', 'AEE', 'AEP', 'AES', 'AFL', 'AIZ', 'AJG', 'AKAM', 'ALB', 'ALGN', 'ALL', 'ALLE', 'AMAT', 'AMCR', 'AMD', 'AME', 'AMGN', 'AMP', 'AMT', 'AMZN', 'ANET', 'ANSS', 'ANTM', 'AON', 'AOS', 'APD', 'APH', 'APTV', 'ARE', 'ATO', 'ATVI', 'AVB', 'AVGO', 'AVY', 'AWK', 'AXP', 'AZO', 'BAC', 'BAX', 'BBY', 'BDX', 'BIO', 'BK', 'BKNG', 'BLK', 'BLL', 'BMY', 'BR', 'BSX', 'BWA', 'C', 'CAT', 'CB', 'CBOE', 'CBRE', 'CCI', 'CDNS', 'CDW', 'CE', 'CERN', 'CF', 'CFG', 'CHD', 'CHRW', 'CHTR', 'CI', 'CINF', 'CL', 'CLX', 'CMA', 'CMCSA', 'CME', 'CMG', 'CMI', 'CMS', 'CNC', 'CNP', 'COF', 'COO', 'COP', 'COST', 'CPRT', 'CRL', 'CRM', 'CSCO', 'CSX', 'CTAS', 'CTLT', 'CTSH', 'CTXS', 'CVX', 'CZR', 'D', 'DE', 'DFS', 'DG', 'DGX', 'DHI', 'DHR', 'DIS', 'DLR', 'DLTR', 'DOV', 'DPZ', 'DRE', 'DRI', 'DTE', 'DUK', 'DVA', 'DXCM', 'EA', 'EBAY', 'ECL', 'ED', 'EFX', 'EL', 'EMN', 'EMR', 'ENPH', 'EQIX', 'EQR', 'ES', 'ESS', 'ETN', 'ETR', 'ETSY', 'EVRG', 'EW', 'EXC', 'EXPD', 'EXR', 'FAST', 'FB', 'FBHS', 'FCX', 'FDX', 'FE', 'FFIV', 'FIS', 'FISV', 'FITB', 'FLT', 'FMC', 'FRC', 'FTNT', 'FTV', 'GD', 'GL', 'GLW', 'GM', 'GNRC', 'GOOG', 'GOOGL', 'GPC', 'GPN', 'GRMN', 'GS', 'GWW', 'HAS', 'HBAN', 'HCA', 'HD', 'HES', 'HIG', 'HLT', 'HOLX', 'HON', 'HPE', 'HPQ', 'HRL', 'HSIC', 'HSY', 'HUM', 'ICE', 'IDXX', 'IEX', 'IFF', 'ILMN', 'INFO', 'INTC', 'INTU', 'IP', 'IPG', 'IPGP', 'IQV', 'IRM', 'ISRG', 'IT', 'ITW', 'J', 'JBHT', 'JCI', 'JKHY', 'JNJ', 'JNPR', 'JPM', 'KEY', 'KEYS', 'KLAC', 'KMB', 'KMX', 'KO', 'KR', 'KSU', 'L', 'LDOS', 'LEN', 'LH', 'LHX', 'LIN', 'LKQ', 'LLY', 'LMT', 'LNC', 'LNT', 'LOW', 'LRCX', 'LYB', 'LYV', 'MA', 'MAA', 'MAR', 'MAS', 'MCD', 'MCHP', 'MCK', 'MCO', 'MDLZ', 'MDT', 'MET', 'MGM', 'MKC', 'MKTX', 'MLM', 'MMC', 'MNST', 'MPC', 'MPWR', 'MRK', 'MS', 'MSCI', 'MSFT', 'MSI', 'MTD', 'MU', 'NDAQ', 'NEE', 'NEM', 'NFLX', 'NI', 'NKE', 'NLOK', 'NOC', 'NOW', 'NRG', 'NSC', 'NTAP', 'NTRS', 'NUE', 'NVDA', 'NVR', 'NWS', 'NWSA', 'NXPI', 'O', 'ODFL', 'OKE', 'ORCL', 'ORLY', 'PAYC', 'PAYX', 'PBCT', 'PCAR', 'PEAK', 'PEG', 'PENN', 'PEP', 'PFE', 'PFG', 'PG', 'PGR', 'PH', 'PHM', 'PKG', 'PKI', 'PLD', 'PM', 'PNC', 'PNR', 'PNW', 'POOL', 'PPG', 'PRU', 'PSA', 'PTC', 'PWR', 'PYPL', 'QCOM', 'QRVO', 'RE', 'REGN', 'RF', 'RHI', 'RJF', 'RL', 'RMD', 'ROK', 'ROL', 'ROP', 'ROST', 'RSG', 'RTX', 'SBAC', 'SBUX', 'SCHW', 'SHW', 'SIVB', 'SNA', 'SNPS', 'SO', 'SPGI', 'SRE', 'STE', 'STT', 'STX', 'STZ', 'SWK', 'SWKS', 'SYF', 'SYK', 'SYY', 'TDG', 'TDY', 'TECH', 'TEL', 'TER', 'TFC', 'TFX', 'TGT', 'TJX', 'TMO', 'TMUS', 'TRMB', 'TROW', 'TRV', 'TSCO', 'TSLA', 'TSN', 'TT', 'TTWO', 'TWTR', 'TXN', 'TXT', 'TYL', 'UDR', 'UHS', 'ULTA', 'UNH', 'UNP', 'UPS', 'URI', 'USB', 'V', 'VFC', 'VMC', 'VRSK', 'VRSN', 'VRTX', 'VZ', 'WAT', 'WEC', 'WELL', 'WHR', 'WLTW', 'WM', 'WMT', 'WRB', 'WST', 'WU', 'WY', 'XEL', 'XLNX', 'XYL', 'YUM', 'ZBH', 'ZBRA', 'ZION', 'ZTS']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jbHDRKUNeWOW",
        "outputId": "1aaa4ba7-3d72-46d5-9d20-b001c28fe533"
      },
      "source": [
        "train_data.shape , valid_data.shape , test_data.shape"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((871, 400), (249, 400), (125, 400))"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bBPYH0ahihP-"
      },
      "source": [
        "'''\n",
        "debug_corr_1_list = []\n",
        "debug_corr_2_list = []\n",
        "debug_corr_3_list = []\n",
        "debug_corr_4_list = []\n",
        "\n",
        "training_loss_list = []\n",
        "best_performance_list = []\n",
        "\n",
        "for cluster in clusters:\n",
        "    before_train = datetime.now().timestamp()\n",
        "    _, normalize_statistic, training_loss, best_performance = train(train_data[cluster-1], valid_data[cluster-1], args, result_train_files[cluster-1])\n",
        "    after_train = datetime.now().timestamp()\n",
        "    print(f'Training took {(after_train - before_train) / 60} minutes')\n",
        "\n",
        "    debug_corr_1_list.append(np.array(debug_corr_1))\n",
        "    debug_corr_2_list.append(np.array(debug_corr_2))\n",
        "    debug_corr_3_list.append(np.array(debug_corr_3))\n",
        "    debug_corr_4_list.append(np.array(debug_corr_4))\n",
        "\n",
        "    training_loss_list.append(training_loss)\n",
        "    best_performance_list.append(best_performance)\n",
        "\n",
        "    debug_step = 0\n",
        "    debug_corr_1 = None\n",
        "    debug_corr_2 = None\n",
        "    debug_corr_3 = None\n",
        "    debug_corr_4 = None\n",
        "\n",
        "before_train = datetime.now().timestamp()\n",
        "_, normalize_statistic, training_loss, best_performance = train(train_data, valid_data, args, result_train_file)\n",
        "after_train = datetime.now().timestamp()\n",
        "print(f'Training took {(after_train - before_train) / 60} minutes')\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CQgu3bzkg_Oq"
      },
      "source": [
        "def print_metrics(performance_metrics):\n",
        "    metrics = ['mae', 'mape', 'rmse']\n",
        "\n",
        "    for metric in metrics:\n",
        "        print(metric, performance_metrics[metric])\n",
        "        print(metric + \"_norm\", performance_metrics[metric + \"_norm\"])\n",
        "\n",
        "\n",
        "#print_metrics(best_performance)\n",
        "#print(\"Normalize statistic\", normalize_statistic)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mQwSzBGkaDMi"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wfOQ9QHrshAQ"
      },
      "source": [
        "def visualize_results(cluster):\n",
        "    prediction = pd.read_csv(f'output/{cluster}/train/predict.csv',header=None)\n",
        "    target = pd.read_csv(f'output/{cluster}/train/target.csv',header=None)\n",
        "    selected_stocks = clusters_partition.loc[:, 'ts1'][clusters_partition[\"cluster\"] == cluster]\n",
        "    indexes = selected_stocks.tolist()\n",
        "    indexes.append('date')\n",
        "    target_data = pd.read_csv(data_file)[indexes].set_index('date').iloc[(train_data[cluster-1].shape[0] + 12):((train_data[cluster-1].shape[0] + 12) + prediction.shape[0]),]\n",
        "    prediction.columns = np.array(target_data.columns) + 'pred'\n",
        "    prediction['date'] = target_data.index\n",
        "    prediction = prediction.set_index('date')\n",
        "    _, ax = plt.subplots(1,2,figsize=(20,10))\n",
        "    ax[0].set_title('Target')\n",
        "    target_data.iloc[:,:10].plot(ax=ax[0])\n",
        "    for line, name in zip(ax[0].lines, target_data.columns):\n",
        "        y = line.get_ydata()[-1]\n",
        "        ax[0].annotate(name, xy=(1,y), xytext=(6,0), color=line.get_color(), \n",
        "                    xycoords = ax[0].get_yaxis_transform(), textcoords=\"offset points\",\n",
        "                    size=14, va=\"center\")\n",
        "\n",
        "\n",
        "    # _, ax = plt.subplots(figsize=(10,10))\n",
        "    prediction.iloc[:,:10].plot(ax=ax[1])\n",
        "    ax[1].set_title('Prediction')\n",
        "    for line, name in zip(ax[1].lines, prediction.columns):\n",
        "        y = line.get_ydata()[-1]\n",
        "        ax[1].annotate(name, xy=(1,y), xytext=(6,0), color=line.get_color(), \n",
        "                    xycoords = ax[1].get_yaxis_transform(), textcoords=\"offset points\",\n",
        "                    size=14, va=\"center\")\n",
        "    ax[1].set_ylim(0,550)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lVAUnVJPB4Hf"
      },
      "source": [
        "import numpy as np\n",
        "import scipy\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "\n",
        "def visualize_map(cluster):\n",
        "    def normalize_plot(mat):\n",
        "        return scipy.stats.zscore(mat, axis=None)\n",
        "\n",
        "    figure = plt.figure(figsize=(8, 8))\n",
        "    plt.subplot(221)\n",
        "    plt.title('self_graph_attention() before leakyrelu')\n",
        "    plt.imshow(normalize_plot(debug_corr_1[0]))\n",
        "\n",
        "    plt.subplot(222)\n",
        "    plt.title('plus leakyrelu')\n",
        "    plt.imshow(normalize_plot(debug_corr_2[0]))\n",
        "\n",
        "    plt.subplot(223)\n",
        "    plt.title('plus softmax(dim=-1) and mean(dim=0)')\n",
        "    plt.imshow(normalize_plot(debug_corr_3))\n",
        "\n",
        "    plt.subplot(224)\n",
        "    plt.title('plus (attention + attention.T)')\n",
        "    plt.imshow(normalize_plot(debug_corr_4))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9XdsTEU-tswP"
      },
      "source": [
        "'''\n",
        "plot_loss(training_loss)\n",
        "visualize_results(cluster=cluster)\n",
        "visualize_map(cluster=cluster)\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VJYAzOtDt35u"
      },
      "source": [
        "'''\n",
        "20 stocks\n",
        "NORM: MAPE 76.330897867%; MAE 1.508740934; RMSE 1.804532450.\n",
        "RAW : MAPE 77.525642904%; MAE 15.531246071; RMSE 18.198214374.\n",
        "Training took 0.6056053519248963 minutes\n",
        "\n",
        "plot_loss(training_loss_list[1])\n",
        "visualize_results(cluster=2)\n",
        "visualize_map(cluster=2)\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mjrOz6fOt7U6"
      },
      "source": [
        "'''\n",
        "71 stocks\n",
        "NORM: MAPE 64.038134969%; MAE 0.865533014; RMSE 1.260219140.\n",
        "RAW : MAPE 20.049844685%; MAE 8.501538595; RMSE 14.978990272.\n",
        "Training took 0.7090381344159444 minutes\n",
        "\n",
        "plot_loss(training_loss_list[2])\n",
        "visualize_results(cluster=3)\n",
        "visualize_map(cluster=3)\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XBEWmfT7458J"
      },
      "source": [
        "'1 stock'\n",
        "'''\n",
        "plot_loss(training_loss_list[3])\n",
        "visualize_results(cluster=4)\n",
        "visualize_map(cluster=4)\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pmZEYxrxMzyD"
      },
      "source": [
        "# aggregate best_performance_list\n",
        "#print(best_performance_list)\n",
        "'''\n",
        "def aggregate(metric, number_of_stocks, total_stocks, best_performance_list):\n",
        "    #print(best_performance_list)\n",
        "    performance = [c[metric] if c is not None else 0 for c in best_performance_list ]\n",
        "    print(metric, performance)\n",
        "    return np.dot(performance, number_of_stocks) / total_stocks\n",
        "\n",
        "metrics = ['mae', 'mape', 'rmse']\n",
        "\n",
        "number_of_stocks = [x.shape[1] for x in train_data]\n",
        "total_stocks = np.sum(number_of_stocks)\n",
        "\n",
        "for metric in metrics:\n",
        "    print(metric)\n",
        "    aggregated_metric = aggregate(metric, number_of_stocks, total_stocks, best_performance_list)\n",
        "    aggregated_metric_norm = aggregate(metric + \"_norm\", number_of_stocks, total_stocks, best_performance_list)\n",
        "    print(\"Metric\", metric, aggregated_metric)\n",
        "    print(\"Metric\", metric + \"_norm\", aggregated_metric_norm)\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9REs5BgLRxng"
      },
      "source": [
        "# test on testing set on most recent model ran\n",
        "'''\n",
        "model = torch.load(os.path.join(result_train_file, '_stemgnn.pt'))\n",
        "\n",
        "test_set = ForecastDataset(test_data, window_size=args.window_size, horizon=args.horizon,\n",
        "                                normalize_method=args.norm_method, norm_statistic=normalize_statistic)\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(test_set, batch_size=args.batch_size, shuffle=False, num_workers=0)\n",
        "\n",
        "node_cnt = train_data.shape[1]\n",
        "\n",
        "performance_metrics = validate(model, test_loader, args.device, args.norm_method, normalize_statistic,\n",
        "                         node_cnt, args.window_size, args.horizon)\n",
        "print_metrics(performance_metrics)\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ItBkURq7Z8si"
      },
      "source": [
        "# Run 1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o_YzlDU6Zq1c"
      },
      "source": [
        "before_train = datetime.now().timestamp()\n",
        "_, normalize_statistic, training_loss, best_performance = train(train_data, valid_data, args, result_train_file)\n",
        "after_train = datetime.now().timestamp()\n",
        "print(f'Training took {(after_train - before_train) / 60} minutes')\n",
        "print_metrics(best_performance)\n",
        "print(\"Normalize statistic\", normalize_statistic)\n",
        "plot_loss(training_loss)\n",
        "visualize_results(cluster=cluster)\n",
        "visualize_map(cluster=cluster)\n",
        "model = torch.load(os.path.join(result_train_file, '_stemgnn.pt'))\n",
        "\n",
        "test_set = ForecastDataset(test_data, window_size=args.window_size, horizon=args.horizon,\n",
        "                                normalize_method=args.norm_method, norm_statistic=normalize_statistic)\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(test_set, batch_size=args.batch_size, shuffle=False, num_workers=0)\n",
        "\n",
        "node_cnt = train_data.shape[1]\n",
        "\n",
        "performance_metrics = validate(model, test_loader, args.device, args.norm_method, normalize_statistic,\n",
        "                         node_cnt, args.window_size, args.horizon)\n",
        "print_metrics(performance_metrics)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XZ8S18HzafJ-"
      },
      "source": [
        "\n",
        "# Run 2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ur26E5sjaioE"
      },
      "source": [
        "before_train = datetime.now().timestamp()\n",
        "_, normalize_statistic, training_loss, best_performance = train(train_data, valid_data, args, result_train_file)\n",
        "after_train = datetime.now().timestamp()\n",
        "print(f'Training took {(after_train - before_train) / 60} minutes')\n",
        "print_metrics(best_performance)\n",
        "print(\"Normalize statistic\", normalize_statistic)\n",
        "plot_loss(training_loss)\n",
        "visualize_results(cluster=cluster)\n",
        "visualize_map(cluster=cluster)\n",
        "model = torch.load(os.path.join(result_train_file, '_stemgnn.pt'))\n",
        "\n",
        "test_set = ForecastDataset(test_data, window_size=args.window_size, horizon=args.horizon,\n",
        "                                normalize_method=args.norm_method, norm_statistic=normalize_statistic)\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(test_set, batch_size=args.batch_size, shuffle=False, num_workers=0)\n",
        "\n",
        "node_cnt = train_data.shape[1]\n",
        "\n",
        "performance_metrics = validate(model, test_loader, args.device, args.norm_method, normalize_statistic,\n",
        "                         node_cnt, args.window_size, args.horizon)\n",
        "print_metrics(performance_metrics)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m-ckud89alDW"
      },
      "source": [
        "# Run 3"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ve41_CZvaoCz"
      },
      "source": [
        "before_train = datetime.now().timestamp()\n",
        "_, normalize_statistic, training_loss, best_performance = train(train_data, valid_data, args, result_train_file)\n",
        "after_train = datetime.now().timestamp()\n",
        "print(f'Training took {(after_train - before_train) / 60} minutes')\n",
        "print_metrics(best_performance)\n",
        "print(\"Normalize statistic\", normalize_statistic)\n",
        "plot_loss(training_loss)\n",
        "visualize_results(cluster=cluster)\n",
        "visualize_map(cluster=cluster)\n",
        "model = torch.load(os.path.join(result_train_file, '_stemgnn.pt'))\n",
        "\n",
        "test_set = ForecastDataset(test_data, window_size=args.window_size, horizon=args.horizon,\n",
        "                                normalize_method=args.norm_method, norm_statistic=normalize_statistic)\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(test_set, batch_size=args.batch_size, shuffle=False, num_workers=0)\n",
        "\n",
        "node_cnt = train_data.shape[1]\n",
        "\n",
        "performance_metrics = validate(model, test_loader, args.device, args.norm_method, normalize_statistic,\n",
        "                         node_cnt, args.window_size, args.horizon)\n",
        "print_metrics(performance_metrics)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gAiY1FaGar5n"
      },
      "source": [
        "# Run 4"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qZdLGTHbatY0"
      },
      "source": [
        "before_train = datetime.now().timestamp()\n",
        "_, normalize_statistic, training_loss, best_performance = train(train_data, valid_data, args, result_train_file)\n",
        "after_train = datetime.now().timestamp()\n",
        "print(f'Training took {(after_train - before_train) / 60} minutes')\n",
        "print_metrics(best_performance)\n",
        "print(\"Normalize statistic\", normalize_statistic)\n",
        "plot_loss(training_loss)\n",
        "visualize_results(cluster=cluster)\n",
        "visualize_map(cluster=cluster)\n",
        "model = torch.load(os.path.join(result_train_file, '_stemgnn.pt'))\n",
        "\n",
        "test_set = ForecastDataset(test_data, window_size=args.window_size, horizon=args.horizon,\n",
        "                                normalize_method=args.norm_method, norm_statistic=normalize_statistic)\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(test_set, batch_size=args.batch_size, shuffle=False, num_workers=0)\n",
        "\n",
        "node_cnt = train_data.shape[1]\n",
        "\n",
        "performance_metrics = validate(model, test_loader, args.device, args.norm_method, normalize_statistic,\n",
        "                         node_cnt, args.window_size, args.horizon)\n",
        "print_metrics(performance_metrics)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S9fa4f8haua2"
      },
      "source": [
        "\n",
        "# Run 5"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mku3VMeCaxNL"
      },
      "source": [
        "before_train = datetime.now().timestamp()\n",
        "_, normalize_statistic, training_loss, best_performance = train(train_data, valid_data, args, result_train_file)\n",
        "after_train = datetime.now().timestamp()\n",
        "print(f'Training took {(after_train - before_train) / 60} minutes')\n",
        "print_metrics(best_performance)\n",
        "print(\"Normalize statistic\", normalize_statistic)\n",
        "plot_loss(training_loss)\n",
        "visualize_results(cluster=cluster)\n",
        "visualize_map(cluster=cluster)\n",
        "model = torch.load(os.path.join(result_train_file, '_stemgnn.pt'))\n",
        "\n",
        "test_set = ForecastDataset(test_data, window_size=args.window_size, horizon=args.horizon,\n",
        "                                normalize_method=args.norm_method, norm_statistic=normalize_statistic)\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(test_set, batch_size=args.batch_size, shuffle=False, num_workers=0)\n",
        "\n",
        "node_cnt = train_data.shape[1]\n",
        "\n",
        "performance_metrics = validate(model, test_loader, args.device, args.norm_method, normalize_statistic,\n",
        "                         node_cnt, args.window_size, args.horizon)\n",
        "print_metrics(performance_metrics)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}